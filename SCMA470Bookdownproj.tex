% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={SCMA470 Risk Analysis and Credibility},
  pdfauthor={Pairote Satiracoo},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{LectureNoteMacro}
\usepackage{bbm}
\usepackage{mathtools}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{SCMA470 Risk Analysis and Credibility}
\author{Pairote Satiracoo}
\date{2021-08-26}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{basic-probability-concepts}{%
\chapter{Basic Probability Concepts}\label{basic-probability-concepts}}

\hypertarget{random-variables}{%
\section{Random Variables}\label{random-variables}}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-1}{}\label{def:unlabeled-div-1}

\emph{Let \(S\) be the sample space of an experiment. A
real-valued function \(X : S \rightarrow \mathbb{R}\) is called a \textbf{random
variable} of the experiment if, for each interval
\(I \subset \mathbb{R}, \, \{s : X(s) \in I \}\) is an event. }

\end{definition}

Random variables are often used for the calculation of the probabilities
of events. The real-valued function \(P(X \le t)\) characterizes \(X\), it
tells us almost everything about \(X\). This function is called the
\textbf{cumulative distribution function} of \(X\). The cumulative distribution
function describes how the probabilities accumulate.

\begin{definition}
\protect\hypertarget{def:unlabeled-div-2}{}\label{def:unlabeled-div-2}

\emph{If \(X\) is a random variable, then the function \(F\)
defined on \(\mathbb{R}\) by \[F(x) = P(X \le x)\] is called the
\textbf{cumulative distribution function} or simply \textbf{distribution function
(c.d.f)} of \(X\).}

\end{definition}

Functions that define the probability measure for discrete and
continuous random variables are the probability mass function and the
probability density function.

\begin{definition}
\protect\hypertarget{def:unlabeled-div-3}{}\label{def:unlabeled-div-3}

\emph{Suppose \(X\) is a discrete random variable. Then the
function \[f(x) = P(X = x)\] that is defined for each \(x\) in the range
of \(X\) is called the \textbf{probability mass function} (p.m.f) of a random
variable \(X\).}

\end{definition}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-4}{}\label{def:unlabeled-div-4}

\emph{Suppose \(X\) is a continuous random variable with
c.d.f \(F\) and there exists a nonnegative, integrable function \(f\),
\(f: \mathbb{R} \rightarrow [0, \infty)\) such that
\[F(x) = \int_{-\infty}^x f(y)\, dy\] Then the function \(f\) is called
the \textbf{probability density function} (p.d.f) of a random variable \(X\).}

\end{definition}

\hypertarget{r-functions-for-probability-distributions}{%
\subsection{R Functions for Probability Distributions}\label{r-functions-for-probability-distributions}}

In R, density, distribution function, for the Poisson distribution with parameter \(\lambda\) is shown as follows:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.12}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.21}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.22}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.21}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.22}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Distribution
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Density function: \(P(X = x)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Distribution function: \(P(X â‰¤ x)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Quantile function (inverse c.d.f.)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
random generation
\end{minipage} \\
\midrule
\endhead
Poisson & \texttt{dpois(x,\ lambda,\ log\ =\ FALSE)} & \texttt{ppois(q,\ lambda,\ lower.tail\ =\ TRUE,\ log.p\ =\ FALSE)} & \texttt{qpois(p,\ lambda,\ lower.tail\ =\ TRUE,\ log.p\ =\ FALSE)} & \texttt{rpois(n,\ lambda)} \\
\bottomrule
\end{longtable}

For the binomial distribution, these functions are pbinom, qbinom, dbinom, and rbinom. For the normal distribution, these functions are pnorm, qnorm, dnorm, and rnorm. And so forth.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{0}\SpecialCharTok{:}\DecValTok{20}
\NormalTok{myData }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{k =} \FunctionTok{factor}\NormalTok{(x), }\AttributeTok{pK =} \FunctionTok{dbinom}\NormalTok{(x, }\DecValTok{20}\NormalTok{, .}\DecValTok{5}\NormalTok{))}
\FunctionTok{ggplot}\NormalTok{(myData,}\FunctionTok{aes}\NormalTok{(k,}\AttributeTok{ymin=}\DecValTok{0}\NormalTok{,}\AttributeTok{ymax=}\NormalTok{pK)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_linerange}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{ylab}\NormalTok{(}\StringTok{"p(k)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{5}\NormalTok{)) }\SpecialCharTok{+}   
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"p.m.f of binomial distribution"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{SCMA470Bookdownproj_files/figure-latex/unnamed-chunk-1-1.pdf}

To plot continuous probability distribution in R, we use stat\_function to add the density function as its arguement. To specify a different mean or standard deviation, we use the \texttt{args} parameter to supply new values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{,}\AttributeTok{by=}\FloatTok{0.1}\NormalTok{))}
\FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+} 
    \FunctionTok{stat\_function}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x),}\AttributeTok{fun=}\NormalTok{dnorm, }\AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{))  }\SpecialCharTok{+} 
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"x"}\NormalTok{, }\AttributeTok{y =} \StringTok{"f(x)"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Normal Distribution With Mean = 0 \& SD = 1"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{SCMA470Bookdownproj_files/figure-latex/unnamed-chunk-2-1.pdf}

\hypertarget{expectation}{%
\section{Expectation}\label{expectation}}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-5}{}\label{def:unlabeled-div-5}

\emph{The \textbf{expected value} of a discrete random variable
\(X\) with the set of possible values \(A\) and probability mass function
\(f(x)\) is defined by \[\mathrm{E}(X) = \sum_{x \in A} x f(x)\]}

\end{definition}

The \textbf{expected value} of a random variable \(X\) is also called the mean,
or the mathematical expectation, or simply the expectation of \(X\). It is
also occasionally denoted by \(\mathrm{E}[X]\), \(\mu_X\), or \(\mu\).

Note that if each value \(x\) of \(X\) is weighted by \(f(x) = P(X = x)\),
then \(\displaystyle \sum_{x \in A} x f(x)\) is nothing but the weighted
average of \(X\).

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-6}{}\label{thm:unlabeled-div-6}

\emph{Let \(X\) be a discrete random variable with set of
possible values \(A\) and probability mass function \(f(x)\), and let \(g\) be
a real-valued function. Then \(g(X)\) is a random variable with
\[\mathrm{E}[g(X)] = \sum_{x \in A} g(x) f(x)\] }

\end{theorem}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-7}{}\label{def:unlabeled-div-7}

\emph{If \(X\) is a continuous random variable with
probability density function \(f\) , the \textbf{expected value} of \(X\) is
defined by \[\mathrm{E}(X) = \int_{-\infty}^\infty x f(x)\, dx\] }

\end{definition}

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-8}{}\label{thm:unlabeled-div-8}

\begin{itemize}
\tightlist
\item
  Let \(X\) be a continuous random variable with
  probability density function \(f (x)\); then for any function
  \(h: \mathbb{R} \rightarrow \mathbb{R}\),
  \[\mathrm{E}[h(X)] = \int_{-\infty}^\infty h(x)\, f(x)\, dx\] *
\end{itemize}

\end{theorem}

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-9}{}\label{thm:unlabeled-div-9}

\emph{Let \(X\) be a random variable. Let
\(h_1, h_2, . . . , h_n\) be real-valued functions, and
\(a_1, a_2, \ldots, a_n\) be real numbers. Then
\[\mathrm{E}[a_1 h_1(X) + a_2 h_2(X) + \cdots + a_n h_n(X)] = a_1 \mathrm{E}[h_1(X)] + a_2 \mathrm{E}[h_2(X)] + \ldots +
    a_n \mathrm{E}[h_n(X)]\]}

\end{theorem}

Moreover, if \(a\) and \(b\) are constants, then
\[\mathrm{E}(aX +b) = a\mathrm{E}(x) + b\]

\hypertarget{variances-of-random-variables}{%
\section{Variances of Random Variables}\label{variances-of-random-variables}}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-10}{}\label{def:unlabeled-div-10}

\emph{Let \(X\) be a discrete random variable with a set of
possible values \(A\), probability mass function \(f(x)\), and
\(\mathrm{E}(X) = \mu\). then \(\mathrm{Var}(X)\) and \(\sigma_X\), called the
\textbf{variance} and \textbf{standard deviation} of \(X\), respectively, are
defined by
\[\mathrm{Var}(X) = \mathrm{E}[(X- \mu)^2] = \sum_{x \in A} (x - \mu)^2 f(x),\]
\[\sigma_X = \sqrt{\mathrm{E}[(X- \mu)^2]}\]}

\end{definition}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-11}{}\label{def:unlabeled-div-11}

\emph{If \(X\) is a continuous random variable with
\(\mathrm{E}(X) = \mu\), then \(\mathrm{Var}(X)\) and \(\sigma_X\), called the
\textbf{variance} and \textbf{standard deviation} of \(X\), respectively, are
defined by
\[\mathrm{Var}(X) = \mathrm{E}[(X- \mu)^2] =  \int_{-\infty}^\infty (x - \mu)^2\, f(x)\, dx ,\]
\[\sigma_X = \sqrt{\mathrm{E}[(X- \mu)^2]}\]}

\end{definition}

We have the following important relations
\[\mathrm{Var}(x) = \mathrm{E}(X^2) - (\mathrm{E}(x))^2 ,\]
\[\mathrm{Var}(aX + b) = a^2\ Var(X), \quad   \sigma_{aX + b}= |a|\sigma_X\]
where \(a\) and \(b\) are constants.

\hypertarget{moments-and-moment-generating-function}{%
\section{Moments and Moment Generating Function}\label{moments-and-moment-generating-function}}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-12}{}\label{def:unlabeled-div-12}

\emph{For \(r > 0\), the \(r\)th moment of \(X\) (the \(r\)th
moment about the origin) is \(\mathrm{E}[X^r]\), when it is defined. The
\(r\)th central moment of a random variable \(X\) (the \(r\)th moment about
the mean) is \(\mathrm{E}[(X - \mathrm{E}[X])^r].\) }

\end{definition}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-13}{}\label{def:unlabeled-div-13}

\emph{The skewness of \(X\) is defined to be the third
central moment, \[\mathrm{E}[(X - \mathrm{E}[X])^3],\] and the
coefficient of skewness to be given by
\[\frac{\mathrm{E}[(X - \mathrm{E}[X])^3]}{(\mathrm{Var}[X])^{3/2}}.\] }

\end{definition}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-14}{}\label{def:unlabeled-div-14}

\emph{The coefficient of kurtosis of \(X\) is defined by
\[\frac{\mathrm{E}[(X - \mathrm{E}[X])^4]}{(\mathrm{Var}[X])^{4/2}}.\] }

\end{definition}

\textbf{Note} In the formula, subtract from the mean and normalise or divide by the standard deviation center and scale to the standard values. Odd-order moments are increased if there is a long tail to the right and decreased if there is a long tail to the left, while even-order moments are increased if either tail is long. A negative value of the coefficient of skewness that the distribution is skewed to the left, or negatively skewed, meaning that the deviations above the mean tend to be smaller than the deviations below the mean, and vice versa. If the coefficent of skewness is close to zero, this could mean symmetry,

\textbf{Note} The fourth moment measures the fatness in the tails, which is always positive. The kurtosis of the standard normal distribution is 3. Using the standard normal distribution as a benchmark, the excess kurtosis of a random variable is defined as the kurtosis minus 3. A higher kurtosis corresponds to a larger extremity of deviations (or outliers), which is called excess kurtosis.

The following diagram compares the shape between the normal distribution and Student's t-distribution. Note that to use the legend with the \texttt{stat\_function} in ggplot2, we use \texttt{scale\_colour\_manual} along with \texttt{colour\ =} inside the \texttt{aes()} as shown below and give names for specific density plots.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{,}\AttributeTok{by=}\FloatTok{0.1}\NormalTok{))}
\FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+} 
    \FunctionTok{stat\_function}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, }\AttributeTok{colour =} \StringTok{"dnorm"}\NormalTok{),}\AttributeTok{fun =}\NormalTok{ dnorm, }\AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{))  }\SpecialCharTok{+} 
    \FunctionTok{stat\_function}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, }\AttributeTok{colour =}\StringTok{"dt"}\NormalTok{),}\AttributeTok{fun =}\NormalTok{ dt, }\AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{df =} \DecValTok{4}\NormalTok{)) }\SpecialCharTok{+}
     \FunctionTok{scale\_colour\_manual}\NormalTok{(}\StringTok{"Legend title"}\NormalTok{, }\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"blue"}\NormalTok{)) }\SpecialCharTok{+} 
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"x"}\NormalTok{, }\AttributeTok{y =} \StringTok{"f(x)"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Normal Distribution With Mean = 0 \& SD = 1"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{SCMA470Bookdownproj_files/figure-latex/unnamed-chunk-3-1.pdf}

Next we will simulate 10000 samples from a normal distribution with mean 0, and standard deviation 1, then compute and interpret for the skewness and kurtosis, and plot the histogram. Here we also use
the function \texttt{set.seed()} to set the seed of R's random number generator, this is useful for creating simulations or random objects that can be reproduced.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{15}\NormalTok{) }\CommentTok{\# Set the seed of R\textquotesingle{}s random number generator}

\CommentTok{\#Simulation}
\NormalTok{n.sample }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{10000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{\#Skewness and Kurtosis}
\FunctionTok{library}\NormalTok{(moments)}
\FunctionTok{skewness}\NormalTok{(n.sample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.03585812
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kurtosis}\NormalTok{(n.sample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.963189
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n.sample),}\FunctionTok{aes}\NormalTok{(x)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{SCMA470Bookdownproj_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{15}\NormalTok{)}

\CommentTok{\#Simulation}
\NormalTok{t.sample }\OtherTok{\textless{}{-}} \FunctionTok{rt}\NormalTok{(}\AttributeTok{n =} \DecValTok{10000}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}

\CommentTok{\#Skewness and Kurtosis}
\FunctionTok{library}\NormalTok{(moments)}
\FunctionTok{skewness}\NormalTok{(t.sample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.06196269
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kurtosis}\NormalTok{(t.sample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.646659
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ t.sample),}\FunctionTok{aes}\NormalTok{(x)) }\SpecialCharTok{+} \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{SCMA470Bookdownproj_files/figure-latex/unnamed-chunk-5-1.pdf}

\textbf{Example} Let us count the number of samples greater than 5 from the samples of the normal and Student's t distributions. Comment on your results

eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFdyaXRlIHlvdXIgY29kZSBoZXJlXG5zZXQuc2VlZCgxNSlcbm4uc2FtcGxlIDwtIHJub3JtKG4gPSAxMDAwMCwgbWVhbiA9IDAsIHNkID0gMSlcbnQuc2FtcGxlIDwtIHJ0KG4gPSAxMDAwMCwgZGYgPSA1KSJ9

\begin{definition}
\protect\hypertarget{def:unlabeled-div-15}{}\label{def:unlabeled-div-15}

\emph{The moment generating function (mgf) of a random
variable \(X\) is defined to be \[M_X(t) = E[e^{tX}],\] if the expectation
exists.}

\end{definition}

\textbf{Note} The moment generating function of \(X\) may not defined (may not
be finite) for all \(t\) in \(\mathbb{R}\).

If \(M_X(t)\) is finite for \(|t| < h\) for some \(h > 0\), then, for any
\(k = 1, 2, \ldots,\) the function \(M_X(t)\) is k-times differentiable at
\(t = 0\), with \[M^{(k)}_X (0) = \mathrm{E}[X^k],\] with
\(\mathrm{E}[|X|^k]\) finite. We can obtain the moments by succesive
differentiation of \(M_X(t)\) and letting \(t = 0\).

\begin{example}
\protect\hypertarget{exm:unlabeled-div-16}{}\label{exm:unlabeled-div-16}

Derive the formula for the mgf of the standard normal distribution.
Hint: its mgf is \(e^{\f t^2}\).

\end{example}

\hypertarget{probability-generating-function}{%
\section{Probability generating function}\label{probability-generating-function}}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-17}{}\label{def:unlabeled-div-17}

\emph{For a counting variable \(N\) (a variable which
assumes some or all of the values \(0, 1, 2, \ldots,\) but no others), The
probability generating function of \(N\) is \[G_N(t) = E[t^N],\] for those
\(t\) in \(\mathbb{R}\) for which the series converges absolutely. }

\end{definition}

Let \(p_k = P(N = k)\). Then
\[G_N(t) = E[t^N] = \sum_{k=0}^\infty t^k p_k.\] It can be shown that if
\(E[N] < \infty\) then \[\mathrm{E}[N] = G'_N(1),\] and if
\(E[N^2] < \infty\) then
\[\mathrm{Var}[N] = G''_N(1) + G'_N(1) - (G'_N(1))^2.\] Moreover, when
both pgf and mgf of \(N\) are defined, we have
\[G_N(t) = M_N(\log(t)) \quad \text{ and } M_N(t) = G_N(e^t).\]

\hypertarget{multivariate-distributions}{%
\section{Multivariate Distributions}\label{multivariate-distributions}}

When \(X_1,X_2,\ldots ,X_n\) be random variables defined on the same
sample space, a multivariate probability density function or probability
mass function\\
\(f(x_1, x_2, \ldots x_n)\) can be defined. The following definitions can
be extended to more than two random variables and the case of discrete
random variables.

\begin{definition}
\protect\hypertarget{def:unlabeled-div-18}{}\label{def:unlabeled-div-18}

\emph{Two random variables \(X\) and \(Y\), defined on the
same sample space, have a continuous joint distribution if there exists
a nonnegative function of two variables, \(f(x, y)\) on
\(\mathbb{R} \times \mathbb{R}\) , such that for any region \(R\) in the
\(xy\)-plane that can be formed from rectangles by a countable number of
set operations, \[P((X, Y)  \in R) = \iint_R f(x,y) \, dx\, dy\] }

\end{definition}

The function \(f (x, y)\) is called the \textbf{joint probability density
function} of \(X\) and \(Y\).

Let \(X\) and \(Y\) have joint probability density function \(f (x, y)\). Let
\(f_Y\) be the probability density function of \(Y\) . To find \(f_Y\) in
terms of \(f\) , note that, on the one hand, for any subset \(B\) of \(R\),
\[P(Y \in B) = \int_B f_Y(y) \, dy,\] and on the other hand, we also
have
\[P(Y \in B) = P(X \in (-\infty, \infty), Y \in B) = \int_B \left(    \int_{-\infty}^\infty f(x,y)\, dx  \right) \, dy.\]

We have
\begin{equation} 
\label{eq:label} f_Y(y) =   \int_{-\infty}^\infty f(x,y)\, dx
\end{equation}
and
\begin{equation} 
\label{eq:label2}  f_X(x) =   \int_{-\infty}^\infty f(x,y)\, dy
\end{equation}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-19}{}\label{def:unlabeled-div-19}

\emph{Let \(X\) and \(Y\) have joint probability density
function \(f (x, y)\); then the functions \(f_X\) and \(f_Y\) in} \eqref{eq:label} and \eqref{eq:label2}
\emph{are called,
respectively, the \textbf{marginal probability density functions} of \(X\) and
\(Y\) .}

\end{definition}

Let \(X\) and \(Y\) be two random variables (discrete, continuous, or
mixed). The \textbf{joint probability distribution function}, or \textbf{joint
cumulative probability distribution function}, or simply the joint
distribution of \(X\) and \(Y\), is defined by
\[F(t, u) = P(X \le t, Y \le u)\] for all \(t, u \in (-\infty, \infty)\).

The marginal probability distribution function of \(X\), \(F_X\), can be
found from \(F\) as follows:
\[F_X(t) = \lim_{n \rightarrow \infty} F(t,u)  = F(t, \infty)\] and
\[F_Y(u) = \lim_{n \rightarrow \infty}F(t,u)  = F( \infty, u)\]

The following relationship between \(f(x,y)\) and \(F(t,u)\) is as follows:
\[F(t,u) = \int_{-\infty}^{u}\int_{-\infty}^{t} f(x,y)\, dx\, dy.\]

We also have
\[\mathrm{E}(X) =   \int_{-\infty}^\infty x f_X(x)\, dx , \quad \mathrm{E}(Y) =   \int_{-\infty}^\infty y f_Y(y)\, dy\]

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-20}{}\label{thm:unlabeled-div-20}

\emph{Let \(f (x, y)\) be the joint probability density function
of random variables \(X\) and \(Y\). If \(h\) is a function of two variables
from \(\mathbb{R}^2\) to \(\mathbb{R}\), then \(h(X, Y )\) is a random
variable with the expected value given by
\[\mathrm{E}[h(X,Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} h(x,y) \, f(x,y)\, dx\, dy\]
provided that the integral is absolutely convergent.}

\end{theorem}

As a consequence of the above theorem, for random variables \(X\) and \(Y\),
\[\mathrm{E}(X + Y) = \mathrm{E}(X) + \mathrm{E}(Y)\]

\hypertarget{independent-random-variables}{%
\section{Independent random variables}\label{independent-random-variables}}

\begin{definition}
\protect\hypertarget{def:unlabeled-div-21}{}\label{def:unlabeled-div-21}

\emph{Two random variables \(X\) and \(Y\) are called
independent if, for arbitrary subsets \(A\) and \(B\) of real numbers, the
events \(\{X \in A\}\) and \(\{Y \in B\}\) are \textbf{independent}, that is, if
\[P(X \in A, Y \in B) = P(X \in A) P(Y \in B).\]}

\end{definition}

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-22}{}\label{thm:unlabeled-div-22}

\emph{Let \(X\) and \(Y\) be two random variables defined on the
same sample space. If \(F\) is the joint probability distribution function
of \(X\) and \(Y\), then \(X\) and \(Y\) are independent if and only if for all
real numbers \(t\) and \(u\), \[F(t,u)  = F_X(t) F_Y(u).\]}

\end{theorem}

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-23}{}\label{thm:unlabeled-div-23}

\emph{Let \(X\) and \(Y\) be jointly continuous random variables
with joint probability density function \(f (x, y)\). Then \(X\) and \(Y\) are
independent if and only if \[f (x, y) = f_X(x) f_Y (y).\]}

\end{theorem}

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-24}{}\label{thm:unlabeled-div-24}

\emph{Let \(X\) and \(Y\) be independent random variables and
\(g : \mathbb{R} \rightarrow\mathbb{R}\) and
\(h : \mathbb{R} \rightarrow\mathbb{R}\) be real-valued functions; then
\(g(X)\) and \(h(Y )\) are also independent random variables.}

\end{theorem}

As a consequence of the above theorem, we obtain

\begin{theorem}
\protect\hypertarget{thm:unlabeled-div-25}{}\label{thm:unlabeled-div-25}

\emph{Let \(X\) and \(Y\) be independent random variables. Then
for all real-value functions \(g : \mathbb{R} \rightarrow\mathbb{R}\) and
\(h : \mathbb{R} \rightarrow\mathbb{R}\),
\[\mathrm{E}[g(X)h(Y)] = \mathrm{E}[g(X)]\mathrm{E}[h(Y)]\] }

\end{theorem}

\hypertarget{conditional-distributions}{%
\section{Conditional Distributions}\label{conditional-distributions}}

Let \(X\) and \(Y\) be two continuous random variables with the joint
probability density function \(f (x, y)\). Note that the case of discrete
random variables can be considered in the same way. When no information
is given about the value of \(Y\), the marginal probability density
function of \(X\), \(f_X(x)\) is used to calculate the probabilities of
events concerning \(X\). However, when the value of \(Y\) is known, to find
such probabilities, \(f_{X|Y} (x|y)\), the conditional probability density
function of \(X\) given that \(Y = y\) is used and is defined as follows:
\[f_{X|Y} (x|y)  = \frac{f(x,y)}{f_Y(y)}\] provided that \(f_Y (y) > 0\).
Note also that the conditional probability density function of \(X\) given
that \(Y = y\) is itseof a probability density function, i.e.
\[\int_{-\infty}^\infty f_{X|Y}(x|y)\, dx  =  1.\]

Note that the conditional probability distribution function of \(X\) given
that \(Y = y\), the conditional expectation of \(X\) given that \(Y = y\) can
be as follows:
\[F_{Y|X}(x|y) = P(X \le x | Y = y) = \int_ {-\infty}^x f_{X|Y}(t|y) \, dt\]
and
\[\mathrm{E}(X|Y = y) =  \int_{-\infty}^{\infty} x  f_{X|Y}(x|y) \, dx,\]
where \(f_Y(y) > 0\).

Note that if \(X\) and \(Y\) are independent, then \(f_{X|Y}\) coincides with
\(f_X\) because
\[f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)} =\frac{f_X(x)f_Y(y)}{f_Y(y)} = f_X(x).\]

\hypertarget{covariance}{%
\section{Covariance}\label{covariance}}

The notion of the variance of a random variable \(X\),
\(\mathrm{Var}(X) = \mathrm{E}[ ( X - \mathrm{E}(X))^2]\) measures the
average magnitude of the fluctuations of the random variable \(X\) from
its expectation, \(\mathrm{E}(X)\). This quantity measures the dispersion,
or spread, of the distribution of \(X\) about its expectation. Now suppose
that \(X\) and \(Y\) are two jointly distributed random variables.
Covariance is a measure of how much two random variables vary together.

Let us calculuate \(\mathrm{Var}(aX + bY)\) the joint spread, or
dispersion, of \(X\) and \(Y\) along the \((ax + by)\)-direction for arbitrary
real numbers \(a\) and \(b\):
\[\mathrm{Var}(aX + bY) = a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) + 2 a b \mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))].\]
However, \(\mathrm{Var}(X)\) and \(\mathrm{Var}(Y )\) determine the
dispersions of \(X\) and \(Y\) independently; therefore,
\(\mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))]\) is the quantity
that gives information about the joint spread, or dispersion, \(X\) and
\(Y\) .

\begin{definition}
\protect\hypertarget{def:unlabeled-div-26}{}\label{def:unlabeled-div-26}

\emph{Let \(X\) and \(Y\) be jointly distributed random
variables; then the \textbf{covariance} of \(X\) and \(Y\) is defined by
\[\mathrm{Cov}(X,Y) =  \mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))].\]}

\end{definition}

Note that for random variables \(X, Y\) and \(Z\), and \(ab > 0\), then the
joint dispersion of \(X\) and \(Y\) along the \((ax + by)\)-direction is
greater than the joint dispersion of \(X\) and \(Z\) along the
\((ax + bz)\)-direction if and only if
\(\mathrm{Cov}(X, Y) > \mathrm{Cov}(X,Z).\)

Note that \[\mathrm{Cov}(X, X) = \mathrm{Var}(X).\] Moreover,
\[\mathrm{Cov}(X,Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y).\]

Properties of covariance are as follows: for arbitrary real numbers
\(a, b, c, d\) and random variables \(X\) and \(Y\),
\[\mathrm{Var}(aX + bY) = a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) + 2 a b \mathrm{Cov}(X,Y).\]
\[\mathrm{Cov}(aX + b, cY + d) = acCov(X, Y)\] For random variables
\(X_1, X_2, . . . , X_n\) and \(Y_1, Y_2, . . . , Y_m\),
\[\mathrm{Cov}(\sum_{i=1}^n a_i X_i, \sum_{j=1}^m b_j Y_j) = \sum_{i=1}^n\sum_{j=1}^m a_i\,b_j\, \mathrm{Cov}(X_i,Y_j).\]

If \(\mathrm{Cov}(X, Y) > 0\), we say that \(X\) and \(Y\) are positively
correlated. If \(\mathrm{Cov}(X, Y) < 0\), we say that they are negatively
correlated. If \(\mathrm{Cov}(X, Y) = 0\), we say that \(X\) and \(Y\) are
uncorrelated.

If \(X\) and \(Y\) are independent, then \[\mathrm{Cov}(X,Y) = 0.\] However,
the converse of this is not true; that is, two dependent random
variables might be uncorrelated.

\hypertarget{correlation}{%
\section{Correlation}\label{correlation}}

A large covariance can mean a strong relationship between variables.
However, we cannot compare variances over data sets with different
scales. A weak covariance in one data set may be a strong one in a
different data set with different scales. The problem can be fixed by
dividing the covariance by the standard deviation to get the correlation
coefficient.

\begin{definition}
\protect\hypertarget{def:unlabeled-div-27}{}\label{def:unlabeled-div-27}

\emph{Let \(X\) and \(Y\) be two random variables with
\(0< \sigma^2_X, \sigma^2_Y < \infty\). The covariance between the
standardized \(X\) and the standardized \(Y\) is called the correlation
coefficient between \(X\) and \(Y\) and is denoted \(\rho = \rho(X,Y)\),
\[\rho(X,Y) = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}.\] }

\end{definition}

Note that

\begin{itemize}
\item
  \(\rho(X, Y ) > 0\) if and only if \(X\) and \(Y\) are positively
  correlated;
\item
  \(\rho(X, Y ) < 0\) if and only if \(X\) and \(Y\) are negatively
  correlated; and
\item
  \(\rho(X, Y ) = 0\) if and only if \(X\) and \(Y\) are uncorrelated.
\item
  \(\rho(X, Y )\) roughly measures the amount and the sign of linear
  relationship between \(X\) and \(Y\).
\end{itemize}

In the case of perfect linear relationship, we have
\(\rho(X, Y ) = \pm1\). A correlation of 0, i.e.~\(\rho(X, Y ) = 0\) does
not mean zero relationship between two variables; rather, it means zero
linear relationship.

Some importants properties of correlation are
\[-1 \le \rho(X, Y ) \le 1\]
\[\rho(a X + b, cY +d) = \text{sign}(ac) \rho(X, Y )\]

\hypertarget{model-fitting}{%
\section{Model Fitting}\label{model-fitting}}

The contents in this section are taken from Gray and Pitts.

To fit a parametric model, we have to calculate estimates of the unknown
parameters of the probability distribution. Various criteria are
available, including the method of moments, the method of maximum
likelihood, etc.

\hypertarget{the-method-of-moments}{%
\section{The method of moments}\label{the-method-of-moments}}

The method of moments leads to parameter estimates by simply matching
the moments of the model,
\(\mathrm{E}[X], \mathrm{E}[X^2], \mathrm{E}[X^3], \ldots ,\) in turn to
the required number of corresponding sample moments calculated from the
data \(x_1, x_2, \ldots , x_n\), where \(n\) is the number of observations
available. The sample moments are simply
\[\frac{1}{n}\sum_{i=1}^n x_i, \quad  \frac{1}{n}\sum_{i=1}^n  x^2_i, \quad \frac{1}{n}\sum_{i=1}^n x^3_i, \ldots.\]
It is often more convenient to match the mean and central moments, in
particular matching \(\mathrm{E}[X]\) to the sample mean \(\bar{x}\) and
\(\mathrm{Var}[X]\) to the sample variance
\[s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2.\]

An estimate produced using the method of moments is called an MME, and
the MME of a parameter \(\theta\), say, is usually denoted
\(\tilde{\theta}\).

\hypertarget{the-method-of-maximum-likelihood}{%
\section{The method of maximum likelihood}\label{the-method-of-maximum-likelihood}}

The method of maximum likelihood is the most widely used method for
parameter estimation. The estimates it produces are those values of the
parameters which give the maximum value attainable by the likelihood
function, denoted \(L\), which is the joint probability mass or density
function for the data we have (under the chosen parametric
distribution), regarded as a function of the unknown parameters.

In practice, it is often easier to maximise the loglikelihood function,
which is the logarithm of the likelihood function, rather than the
likelihood itself. An estimate produced using the method of maximum
likelihood is called an MLE, and the MLE of a parameter \(\theta\), say,
is denoted \(\hat{\theta}\). MLEs have many desirable theoretical
properties, especially in the case of large samples.

In some simple cases we can derive MLE(s) analytically as explicit
functions of summaries of the data. Thus, suppose our data consist of a
random sample \(x_1, x_2, \ldots , x_n\), from a parametric distribution
whose parameter(s) we want to estimate. Some straightforward cases
include the following:

\begin{itemize}
\item
  the MLE of \(\lambda\) for a \(Poi(\lambda)\) distribution is the sample
  mean, that is \(\hat{\lambda} = \bar{x}\)
\item
  the MLE of \(\lambda\) for an \(Exp(\lambda)\) distribution is the
  reciprocal of the sample mean, that is \(\hat{\lambda} = 1/\bar{x}\)
\end{itemize}

\hypertarget{goodness-of-fit-tests}{%
\section{Goodness of fit tests}\label{goodness-of-fit-tests}}

We can assess how well the fitted distributions reflect the distribution
of the data in various ways. We should, of course, examine and compare
the tables of frequencies and, if appropriate, plot and compare
empirical distribution functions. More formally, we can perform certain
statistical tests. Here we will use the Pearson chi-square
goodness-of-fit criterion.

\hypertarget{the-pearson-chi-square-goodness-of-fit-criterion}{%
\section{the Pearson chi-square goodness-of-fit criterion}\label{the-pearson-chi-square-goodness-of-fit-criterion}}

We construct the test statistic \[\chi^2 = \frac{\sum(O - E)^2}{E},\]
where \(O\) is the observed frequency in a cell in the frequency table and
\(E\) is the fitted or expected frequency (the frequency expected in that
cell under the fitted model), and where we sum over all usable cells.

\textbf{The null hypothesis} is that the sample comes from a specified
distribution.

The value of the test statistic is then evaluated in one of two ways.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We convert it to a \(P\)-value, which is a measure of the strength of
  the evidence against the hypothesis that the data do follow the
  fitted distribution. \textbf{If the \(P\)-value is small enough, we conclude
  that the data do not follow the fitted distribution -- we say ``the
  fitted distribution does not provide a good fit to the data'' (and
  quote the \(P\)-value in support of this conclusion)}.
\item
  We compare it with values in published tables of the distribution
  function of the appropriate \(\chi^2\) distribution, and if the value
  of the statistic is high enough to be in a tail of specified size of
  this reference distribution, we conclude that the fitted
  distribution does not provide a good fit to the data.
\end{enumerate}

\hypertarget{kolmogorov-smirnov-k-s-test.}{%
\section{Kolmogorov-Smirnov (K-S) test.}\label{kolmogorov-smirnov-k-s-test.}}

The K-S test statistic is the maximum difference between the values of
the ecdf of the sample and the cdf of the fully specified fitted
distribution.

The course does not emphasis on the Goodness of Fit Test. Please refer
to the reference text for more details.

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

The aim of the course is to provide a fundamental basis which applies
mainly in general insurance. General insurance companies' products are
short-term policies that can be purchased for a short period of time.
Examples of insurance products are

\begin{itemize}
\item
  motor insurance;
\item
  home insurance;
\item
  health insurance; and
\item
  travel insurance.
\end{itemize}

In case of an occurrence of an insured event, two important components
of financial losses which are of importance for management of an
insurance company are

\begin{itemize}
\item
  the number of claims; and
\item
  the amounts of those claims.
\end{itemize}

Mathematical and statistical techniques used to model these sources of
uncertainty will be discussed. This will enable insurance companies to

\begin{itemize}
\item
  calculate premium rates to charge policy holders; and
\item
  decide how much reserve should be set aside for the future payment
  of incurred claims.
\end{itemize}

\hypertarget{loss-distributions}{%
\chapter{Loss distributions}\label{loss-distributions}}

In the section, statistical distributions and their properties which are
suitable for modelling claim sizes are reviewed. These distribution are
also known as loss distributions. In practice, the shape of loss
distributions are positive skew with a long right tail. The main
features of loss distributions include:

\begin{itemize}
\item
  having a few small claims;
\item
  rising to a peak;
\item
  tailing off gradually with a few very large claims.
\end{itemize}

\hypertarget{exponential-distribution}{%
\section{Exponential Distribution}\label{exponential-distribution}}

A random variable \(X\) has an exponential distribution with a parameter
\(\lambda > 0\), denoted by \(X \sim \text{Exp}(\lambda)\) if its
probability density function is given by
\[f_X(x) = \lambda e^{-\lambda x}, \quad x > 0.\]

\begin{example}
\protect\hypertarget{exm:unlabeled-div-28}{}\label{exm:unlabeled-div-28}

\textbf{Example 1}. \emph{Let \(X \sim \text{Exp}(\lambda)\) and \(0 < a < b\).}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Find the distribution \(F_X(x)\).}
\item
  \emph{Express \(P(a < X < B)\) in terms of \(f_X(x)\) and \(F_X(x)\).}
\item
  \emph{Show that the moment generating function of \(X\) is
  \[M_X(t) = \left(1 -  \frac{t}{\lambda}\right)^{-1}, \quad t < \lambda.\]}
\item
  \emph{Derive the \(r\)-th moment about the origin \(\mathrm{E}[X^r].\)}
\item
  \emph{Derive the coefficient of skewness for \(X\).}
\item
  \emph{Simulate a random sample of size n = 200 from
  \(X \sim \text{Exp}(0.5)\) using the command
  \texttt{sample\ =\ rexp(n,\ \textbackslash{}lambda)} where \(n\) and \(\lambda\) are the chosen
  parameter values.}
\item
  \emph{Plot a histogram of the random sample using the command
  \texttt{hist(sample)} (use help for available options for \texttt{hist} function
  in R).}
\end{enumerate}

\end{example}

The code for questions 6 and 7 are given below. The histogram is shown
in Figure \protect\hyperlink{FigHistogramExp}{1}.

\begin{verbatim}
# Example 1(6-7)
# set.seed is used so that random number generated from different simulations are the same. 
# The number 5353 can be set arbitrarily. 
set.seed(5353)

nsample <- 200
data_exp <- rexp(nsample, rate = 0.5)

dataset <- data_exp
hist(dataset, breaks=100,probability = TRUE, xlab = "claim sizes" 
     , ylab = "density", main = paste("Histogram of claim sizes" ))

hist(dataset, breaks=100, xlab = "claim sizes" 
     , ylab = "count", main = paste("Histogram of claim sizes" ))
%\end{MyVerbatim}
\end{verbatim}

\begin{figure}
\hypertarget{FigHistogramExp}{%
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{FigHistogramExp.eps}
\caption{\textbf{The histogram of the simulated random samples from
\(X \sim \text{Exp}(0.5)\)}}\label{FigHistogramExp}
}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The exponential distribution can used to model the inter-arrival
  time of an event.
\item
  The exponential distribution has an important property called \textbf{lack
  of memory}: if \(X \sim \text{Exp}(\lambda)\), then the random
  variable \(X-w\) conditional on \(X > w\) has the same distribution as
  \(X\), i.e.
  \[X \sim \text{Exp}(\lambda)\Rightarrow  X - w | X > w \sim \text{Exp}(\lambda).\]
\end{enumerate}

\hypertarget{gamma-distribution}{%
\section{Gamma distribution}\label{gamma-distribution}}

A random variable \(X\) has a gamma distribution with parameters
\(\alpha > 0\) and \(\lambda > 0\), denoted by
\(X \sim \mathcal{G}(\alpha, \lambda)\) or
\(X \sim \text{gamma}(\alpha, \lambda)\) if its probability density
function is given by
\[f_X(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha -1} e^{-\lambda x}, \quad x > 0.\]
The symbol \(\Gamma\) denotes the gamma function, which is defined as
\[\Gamma(\alpha) = \int_{0}^\infty x^{\alpha - 1} e^{-x} \mathop{}\!dx, \quad \text{for } \alpha > 0.\]
It follows that \(\Gamma(\alpha + 1) = \alpha \Gamma(\alpha)\) and that
for a positive integer \(n\), \(\Gamma(n) = (n-1)!\).

The properties of the gamma distribution are summarised.

\begin{itemize}
\item
  The mean and variance of \(X\) are
  \[\mathrm{E}[X] = \frac{\alpha}{\lambda} \text{ and } \mathrm{Var}[X] =\frac{\alpha}{\lambda^2}\]
\item
  The \(r\)-th moment about the origin is
  \[\mathrm{E}[X^r] = \frac{1}{\lambda^r} \frac{\Gamma(\alpha + r)}{\Gamma(\alpha )}, \quad r > 0.\]
\item
  The moment generating function (mgf) of \(X\) is
  \[M_X(t) = \left(1 -  \frac{t}{\lambda}\right)^{-\alpha}, \quad t < \lambda.\]
\item
  The coefficient of skewness is \[\frac{2}{\sqrt{\alpha}}.\]
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The exponential function is a special case of the gamma
  distribution, i.e.~\(\text{Exp}(\lambda)= \mathcal{G}(1,\lambda)\)
\item
  If \(\alpha\) is a positive integer, the sum of \(\alpha\) independent,
  identically distributed as \(\text{Exp}(\lambda)\), is
  \(\mathcal{G}(\alpha, \lambda)\).
\item
  If \(X_1, X_2, \ldots, X_n\) are independent, identically distributed,
  each with a \(\mathcal{G}(\alpha, \lambda)\) distribution, then
  \[\sum_{i = 1}^n X_i \sim \mathcal{G}(n\alpha, \lambda).\]
\item
  The exponential and gamma distributions are not fat-tailed, and
  \textbf{may not provide a good fit} to claim amounts.
\end{enumerate}

\begin{example}
\protect\hypertarget{exm:unlabeled-div-29}{}\label{exm:unlabeled-div-29}

\textbf{Example 2}. \emph{Using the moment generating function of a gamma
distribution, show that the sum of independent gamma random variables
with the same scale parameter \(\lambda\),
\(X \sim \mathcal{G}(\alpha_1, \lambda)\) and
\(Y \sim \mathcal{G}(\alpha_2, \lambda)\), is
\(S = X+ Y \sim \mathcal{G}(\alpha_1 + \alpha_2, \lambda).\)}

\end{example}

Because \(X\) and \(Y\) are independent, \[\begin{aligned}
    M_S(t) &= M_{X+Y}(t) = M_X(t) \cdot M_Y(t)\\
        &= (1 - \frac{t}{\lambda})^{-\alpha_1} \cdot (1 - \frac{t}{\lambda})^{-\alpha_2}     \\
        &=  (1 - \frac{t}{\lambda})^{-(\alpha_1 + \alpha_2)}. \end{aligned}\]
Hence \(S = X + Y \sim \mathcal{G}(\alpha_1 + \alpha_2, \lambda).\)

\begin{example}
\protect\hypertarget{exm:exampleFittingClaimSizes}{}\label{exm:exampleFittingClaimSizes}

\textbf{Example 3}. \emph{Consider a data set consisting of 200 claim amounts in
one year from a general insurance portfolio.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Calculate the sample mean and sample standard deviation.}
\item
  \emph{Use the method of moments to fit these data with both exponential
  and gamma distributions.}
\item
  \emph{Calculate the boundaries for groups or bins so that the expected
  number of claims in each bin is 20 under the fitted exponential
  distribution.}
\item
  \emph{Count the values of the observed claim amounts in each bin.}
\item
  \emph{With these bin boundaries, find the expected number of claims when
  the data are fitted with the gamma distribution.}
\item
  \emph{Plot two histograms for the data set with fitted exponential
  distribution and the dataset with fitted gamma distribution.}
\item
  \emph{Comment on the goodness of fit of the fitted distributions.}
\end{enumerate}

\end{example}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Given that \(\sum_{i=1}^n x_i = 206046.4\) and
  \(\sum_{i=1}^n x_i^2 = 1,472,400,135\), we have
  \[\bar{x} = \frac{\sum_{i=1}^n x_i}{n} = \frac{206046.4}{200} = 1030.232.\]
  The sample variance and standard deviation are
  \[s^2 = \frac{1}{n-1} \left( \sum_{i=1}^n x_i^2 - \frac{(\sum_{i=1}^n x_i)^2}{n} \right) = 5135.093,\]
  and \[s = 2516.403.\]
\item
  \protect\hypertarget{example2}{}{\[example2\]} We calculate estimates of
  unknown parameters of both exponential and gamma distributions by
  the method of moments. We simply match the mean and central moments,
  i.e.~matching \(\mathrm{E}[X]\) to the sample mean \(\bar{x}\) and
  \(\mathrm{Var}[X]\) to the sample variance.

  The MME (moment matching estimation) of the required distributions
  are as follows:

  \begin{itemize}
  \item
    the MME of \(\lambda\) for an \(\text{Exp}(\lambda)\) distribution
    is the reciprocal of the sample mean,
    \[\tilde{\lambda} = \frac{1}{\bar{x}} = 0.000971.\]
  \item
    the MMEs of \(\alpha\) and \(\lambda\) for a
    \(\mathcal{G}(\alpha, \lambda)\) distribution are
    \[\begin{aligned}
    \tilde{\alpha} &= \left(\frac{\bar{x}}{s}\right)^2 = 0.167614, \\
    \tilde{\lambda} &= \frac{\tilde{\alpha}}{\bar{x}} = 0.000163.\end{aligned}\]
  \item
    the MMEs of \(\mu\) and \(\sigma\) for a
    \(\mathcal{LN}(\mu, \sigma^2)\) distribution are \[\begin{aligned}
    \tilde{\sigma} &= \sqrt{ \ln \left(  \frac{s^2}{\bar{x}^2} + 1 \right)  }  = 1.393218, \\
    \tilde{\mu} &= \ln(\bar{x}) - \frac{\tilde{\sigma}^2 }{2} = 5.967012.\end{aligned}\]
  \item
    the MMEs of \(\alpha\) and \(\lambda\) for a
    \(\text{Pa}(\alpha, \lambda)\) distribution are \[\begin{aligned}
    \tilde{\alpha} &= \displaystyle{ 2 \left(  \frac{s^2}{\bar{x}^2} \right) \frac{1}{(\frac{s^2}{\bar{x}^2} - 1)}   } = 2.402731,\\
    \tilde{\lambda} &= \bar{x} (\tilde{\alpha} - 1) = 1445.138.\end{aligned}\]
  \end{itemize}
\item
  The upper boundaries for the 10 groups or bins so that the expected
  number of claims in each bin is 20 under the fitted exponential
  distribution are determined by
  \[\Pr(X \le \text{upbd}_j) = frac{j}{10}, \quad j = 1,2,3, \ldots, 9.\]
  With \(\tilde{\lambda}\) from the MME for an \(\text{Exp}(\lambda)\) in
  \protect\hyperlink{example2}{\[example2\]},
  \[\Pr(X \le x)  = 1 - \exp(-\tilde{\lambda} x).\] We obtain
  \[\text{upbd}_j = -\frac{1}{\tilde{\lambda}} \ln\left( 1 - \frac{j}{10}\right).\]
  The results are given in Table
  \protect\hyperlink{tableFitted}{1}.
\item
  The following table shows frequency distributions for observed and
  fitted claims sizes for exponential, gamma, lognormal and Pareto
  fits.

  \hypertarget{tableFitted}{}
  \begin{longtable}[]{@{}lcrrrr@{}}
  \caption{Frequency distributions for observed and fitted claims sizes}\tabularnewline
  \toprule
  & Observation & Exp & Gamma & Lognormal & Pareto \\
  \midrule
  \endfirsthead
  \toprule
  & Observation & Exp & Gamma & Lognormal & Pareto \\
  \midrule
  \endhead
  (0,109{]} & 60 & 20 & 109.4 & 36 & 31.9 \\
  (109,230{]} & 31 & 20 & 14.3 & 34.4 & 27.8 \\
  (230,367{]} & 25 & 20 & 9.7 & 26 & 24.2 \\
  (367,526{]} & 17 & 20 & 7.8 & 20.5 & 21.2 \\
  (526,714{]} & 14 & 20 & 6.8 & 16.6 & 18.6 \\
  (714,944{]} & 13 & 20 & 6.3 & 13.9 & 16.4 \\
  (944,1240{]} & 6 & 20 & 6.2 & 11.9 & 14.6 \\
  (1240,1658{]} & 7 & 20 & 6.5 & 10.8 & 13.2 \\
  (1658,2372{]} & 10 & 20 & 7.7 & 10.4 & 12.5 \\
  (2372,\(\infty\)) & 17 & 20 & 25.4 & 19.5 & 19.4 \\
  \bottomrule
  \end{longtable}

  \protect\hypertarget{tableFitted}{}{\[tableFitted\]}
\item
  Let \(X\) be the claim size.

  \begin{itemize}
  \item
    The expected number of claims for the fitted exponential
    distribution in the range \((a,b]\) is
    \[200 \cdot \Pr( a < X \le b) = 200( e^{-\tilde{\lambda} a} - e^{-\tilde{\lambda} b} ).\]
    In our case, the expected frequencies under the fitted
    exponential distribution are given in the third column of Table
    \protect\hyperlink{tableFitted}{1}.
  \item
    The expected number of claims for the fitted gamma distribution
    in the range \((a,b]\) is
    \[200 \cdot\left(  \text{GAMMADIST}\left(b, \tilde{\alpha}, \frac{1}{\tilde{\lambda}}, \text{TRUE}\right)  - \text{GAMMADIST}\left(a, \tilde{\alpha}, \frac{1}{\tilde{\lambda}}, \text{TRUE}\right) \right).\]
    The expected frequencies under the fitted gamma distribution are
    given in the fourth column of Table
    \protect\hyperlink{tableFitted}{1}.
  \item
    For the fitted lognormal, the expected number of claims in the
    range \((a,b]\) can be obtained from
    \[200 \cdot\left(  \text{NORMDIST} \left(\frac{LN(b) - 
            \tilde{\mu}}{\tilde{\sigma}}\right)  - \text{NORMDIST}\left(\frac{LN(a) - 
            \tilde{\mu}}{\tilde{\sigma}}\right) \right).\]
  \item
    For the fitted Pareto distribution, the expected number of
    claims in the range \((a,b]\) can be obtained from
    \[200 \left[    \left(\frac{\tilde{\lambda}}{\tilde{\lambda} + a} \right)^{\tilde{\alpha}}  - \left(\frac{\tilde{\lambda}}{\tilde{\lambda} + b} \right)^{\tilde{\alpha}}   \right].\]
  \end{itemize}
\item
  The histograms for the data set with fitted exponential distribution
  and the dataset with fitted gamma distribution are shown in Figures
  \protect\hyperlink{FittedExpGamma}{2} and
  \protect\hyperlink{FittedLognormalPareto}{3}.
\item
  Comments:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    The high positive skewness of the sample reflects the fact that
    SD is large when compared to the mean. Consequently, the
    exponential distribution may not fit the data well.
  \item
    Five claims (2.5\%) are greater than 10,000, which is one of the
    main features of the loss distribution.
  \item
    The fit is poor for the exponential distribution, as we see that
    the model under-fits the data for small claims up to 367 and
    over-fits for large claims between 944 to 2372. The gamma fit is
    again poor. We see that the model over-fits for small claims
    between 0-109 and under-fits for claims 230 and 944.
  \item
    Which one of the lognormal and Pareto distributions provides a
    better fit to the observed claim data?
  \end{enumerate}
\end{enumerate}

\begin{figure}
\hypertarget{FittedExpGamma}{%
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{Example3ClaimSizesExpoGamma.pdf}
\caption{\textbf{Histogram of claim sizes with fitted exponential and gamma
distributions.}}\label{FittedExpGamma}
}
\end{figure}

\begin{figure}
\hypertarget{FittedLognormalPareto}{%
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{Example3ClaimSizesParetoLognormal.pdf}
\caption{\textbf{Histogram of claim sizes with fitted lognormal and Pareto
distributions.}}\label{FittedLognormalPareto}
}
\end{figure}

\hypertarget{lognormal-distribution}{%
\section{Lognormal distribution}\label{lognormal-distribution}}

A random variable \(X\) has a lognormal distribution with parameters \(\mu\)
and \(\sigma^2\), denoted by \(X \sim \mathcal{LN}(\mu, \sigma^2)\) if its
probability density function is given by
\[f_X(x) = \frac{1}{\sigma x \sqrt{2 \pi}} \exp\left(-\frac{1}{2} \left( \frac{\log(x) - \mu}{\sigma} \right)^2 \right) , \quad x > 0.\]

The following relation holds:
\[X \sim \mathcal{LN}(\mu, \sigma^2)\Leftrightarrow Y = \log X \sim \mathcal{N}(\mu, \sigma^2).\]

The properties of the lognormal distribution are summarised.

\begin{itemize}
\item
  The mean and variance of \(X\) are
  \[\mathrm{E}[X] = \exp\left(\mu + \frac{1}{2} \sigma^2 \right) \text{ and } \mathrm{Var}[X] =\exp\left(2\mu +  \sigma^2 \right) (\exp(\sigma^2) - 1).\]
\item
  The \(r\)-th moment about the origin is
  \[\mathrm{E}[X^r] =\exp\left(r\mu +  \frac{1}{2}r^2 \sigma^2 \right).\]
\item
  The moment generating function (mgf) of \(X\) is not finite for any
  positive value of \(t\).
\item
  The coefficient of skewness is
  \[(\exp(\sigma^2)  + 2) \left(\exp(\sigma^2)  -1 \right)^{1/2} .\]
\end{itemize}

\hypertarget{pareto-distribution}{%
\section{Pareto distribution}\label{pareto-distribution}}

A random variable \(X\) has a Pareto distribution with parameters
\(\alpha > 0\) and \(\lambda > 0\), denoted by
\(X \sim \text{Pa}(\alpha, \lambda)\) if its probability density function
is given by
\[f_X(x) = \frac{\alpha \lambda^\alpha}{(\lambda + x)^{\alpha + 1}}, \quad x > 0.\]
The distribution function is given by
\[F_X(x) = 1 - \left(  \frac{\lambda}{\lambda + \alpha} \right)^\alpha, \quad x > 0.\]

The properties of the gamma distribution are summarized.

\begin{itemize}
\item
  The mean and variance of \(X\) are
  \[\mathrm{E}[X] = \frac{\lambda}{\alpha - 1}, \alpha > 1 \text{ and } \mathrm{Var}[X] = \frac{\alpha \lambda^2}{(\alpha - 1)^2(\alpha - 2)}, \alpha > 2.\]
\item
  The \(r\)-th moment about the origin is
  \[\mathrm{E}[X^r] =\frac{\Gamma(\alpha-r) \Gamma(1+ r)}{\Gamma(\alpha)} \lambda^r, \quad 0 < r < \alpha.\]
\item
  The moment generating function (mgf) of \(X\) is not finite for any
  positive value of \(t\).
\item
  The coefficient of skewness is
  \[\frac{2(\alpha + 1)}{\alpha - 3} \sqrt{\frac{\alpha-2}{\alpha}} , \quad \alpha > 3.\]
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The following conditional tail property for a Pareto distribution is
  useful for reinsurance calculation. Let
  \(X \sim \text{Pa}(\alpha, \lambda)\). Then the random variable
  \(X - w\) conditional on \(X > w\) has a Pareto distribution with
  parameters \(\alpha\) and \(\lambda + w\), i.e.
  \[X \sim \text{Pa}(\alpha, \lambda)\Rightarrow  X - w | X > w \sim \text{Pa}(\alpha,\lambda + w).\]
\item
  The lognormal and Pareto distributions, in practice, provide a
  better fit to claim amounts than exponential and gamma
  distributions.
\item
  Other loss distribution are useful in practice including \textbf{Burr,
  Weibull and loggamma distributions}.
\end{enumerate}

\begin{example}
\protect\hypertarget{exm:unlabeled-div-30}{}\label{exm:unlabeled-div-30}

\textbf{Example 4}. \emph{Consider the data set in Example~
\protect\hyperlink{exampleFittingClaimSizes}{Example~3} and those bin boundaries,}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Find the expected number of claims when the data are fitted with
  lognormal and Pareto distributions.}
\item
  \emph{Plot two histograms for the data set with fitted lognormal
  distribution and the dataset with fitted Pareto distribution.}
\item
  \emph{Comment on the goodness of fit of the fitted distributions.}
\end{enumerate}

\end{example}

The following code creates plots of loss distributions in R using
ggplot2. See the following link for more information
\url{http://t-redactyl.io/blog/2016/03/creating-plots-in-r-using-ggplot2-part-9-function-plots.html}

\begin{verbatim}
# Use ggplots to plot loss distributions

library(ggplot2)

ggplot(data.frame(x=c(0,10)), aes(x=x)) +
  labs(y="Probability density", x = "x") + 
  ggtitle("Exponential distributions")  +
  theme(plot.title = element_text(hjust = 0.5)) +
  stat_function(fun=dexp,geom ="line", args = (mean=0.5), aes(colour = "0.5")) +
  stat_function(fun=dexp,geom ="line", args = (mean=1), aes(colour = "1")) +
  stat_function(fun=dexp,geom ="line", args = (mean=1.5), aes(colour = "1.5")) +
  stat_function(fun=dexp,geom ="line", args = (mean=2), aes(colour = "2")) +
  #scale_colour_manual(expression(paste(lambda, "= ")), values = c("red", "blue", "green", "orange"))
  scale_colour_manual(expression(paste("lambda = ")), values = c("red", "blue", "green", "orange"))

ggplot(data.frame(x=c(0,20)), aes(x=x)) +
  labs(y="Probability density", x = "x") + 
  ggtitle("Gamma distribution")  +
  theme(plot.title = element_text(hjust = 0.5)) +
  stat_function(fun=dgamma, args=list(shape=2, rate=1), aes(colour = "2")) +
  stat_function(fun=dgamma, args=list(shape=6, rate=1) , aes(colour = "6")) +
  scale_colour_manual(expression(paste("rate = 1 and shape = ")), values = c("red", "blue"))

ggplot(data.frame(x=c(0,10)), aes(x=x)) +
  labs(y="Probability density", x = "x") + 
  ggtitle("lognormal distribution")  +
  theme(plot.title = element_text(hjust = 0.5)) +
  stat_function(fun=dlnorm, args = list(meanlog = 0, sdlog = 0.25), aes(colour = "0.25")) +
  stat_function(fun=dlnorm, args = list(meanlog = 0, sdlog = 1), aes(colour = "1")) +
  scale_colour_manual(expression(paste("mu = 0 and sigma = ")), values = c("red", "blue"))

%\end{MyVerbatim}
\end{verbatim}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{FigExp.eps}
\caption{\textbf{The probability density functions (pdf) of exponential
distributions with various parameters \(\lambda\).}}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{FigGamma.eps}
\caption{\textbf{The probability density functions (pdf) of gamma distributions with
various shape parameters \(\alpha\) and rate parameter \(\lambda = 1\).}}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{FigLognormal.eps}
\caption{\textbf{The probability density functions (pdf) of lognormal distributions
with \(\mu = 0\) and \(\sigma = 0.25\) or 1.}}
\end{figure}

The following code uses to obtain the results in
Example~\protect\hyperlink{exampleFittingClaimSizes}{Example~3}.

\begin{verbatim}
#LectureNote_Example3_Loss_fitting_student_version
# by Pairote Satiracoo
# Version 1.04
# Date : 18/08/2020

# References(s)
# http://www.stat.yale.edu/Courses/1997-98/101/chigf.htm
# https://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf
# https://www.unf.edu/~cwinton/html/cop4300/s09/class.notes/c3-GoodnessofFitTests.pdf
# https://bookdown.org/ekothe/navarro26/chisquare.html
# https://statisticsbyjim.com/hypothesis-testing/identify-distribution-data/
# http://spots.gru.edu/nsmith12/openstats/Chisquare_R.pdf

# We use MLE to estimate model parameters.

library(actuar)
library(stats)
library(MASS)
library(fitdistrplus)

set.seed(5353)

nsample <- 200
#data_gamma <- rgamma(nsample, shape =2, rate = 0.02)
data_pareto <- rpareto(nsample, shape =1.5, scale = 500)

#dataset1 <- data_gamma
dataset2 <- data_pareto

dataset <- dataset2
hist(dataset, breaks=100,probability = TRUE, xlab = "claim sizes" 
     , ylab = "density", main = paste("Histogram of claim sizes" ))

summary(dataset)
xbar <- mean(dataset)
var(dataset)
# sqrt(var(dataset))
s <- sd(dataset)

# Maximum-likelihood Fitting of Univariate Distributions
# fitgm <- fitdistr(dataset, dgamma,  list(shape = 1, rate = 0.5), lower=0.001 )

# alpha_hat_gm <- fitgm$estimate[1]
# lambda_hat_gm <- fitgm$estimate[2]

#fitgm

#ks.test(data_gamma, "pgamma", shape = 2.1306319446, rate = 0.0209186022 )
#ks.test(data_gamma, "pgamma", shape = fitgm$estimate[1], rate = fitgm$estimate[2] )
#ks.test(data_gamma, "pgamma", fitgm$estimate[1],fitgm$estimate[2])


################################################################################

# Exponential: X ~ exp(lambda)

# Fit the data set with exponential distribution
dist = "exponential"

# MME is 1/xbar
1/mean(dataset)  # rate parameter for exp dist, rate =  0.0003402187

# Plot histogram of claim sizes
hist(dataset,breaks=100,probability = TRUE, xlab = "claim sizes" 
     , ylab = "density", main = paste("Histogram of claim sizes with fitted", dist ,"distribution" ))

# Add a plot of the density of fitted exponential distribution
curve(dexp(x ,rate = 1/mean(dataset)   ), add = TRUE, col = "blue", lwd = 2)

# density() computes kernel density estimates. 
# Its default method does so with the given kernel and bandwidth for univariate observations.
lines(density(dataset), col = "red", lwd = 2)
legend(5000, 0.0015, legend=c("density()", "fitted exp dist"), lty = c(1,1), lwd = 2, col=c("red","blue"))

# Comparison of empirical distribution function (ecdf) and the cdf of the fitted distribution
plot(ecdf(dataset),
     main="Compare ecdf with estimated cdf", xlab="claim sizes", lty=1)
curve(pexp(x, rate = 1/mean(dataset) ), add=TRUE, col = "blue", lwd = 2)
legend(10000, 0.8, legend=c("ecdf", "estimated cdf"), lty=1, col = c("black","blue"), lwd = 2)

# Calculate the boundaries for groups or bins so that 
# the expected number of claims in each bin is 20 under the fitted exponential distribution. 

#upbd (upper boundary of bins) is required in order to construct a grouped frequency
# of equi-probable groups determined by the fitted expo dist.
j = 0:9
upbd = qexp(j/10, 1/mean(dataset))  #1/mean(dataset) gives the parameter of exponential dist
out <- cut(dataset, breaks = upbd, dig.lab=5)
table(out)
# To fix the class interval, i.e. (the upbd when j = 9,infinity)
upbd[length(upbd)+1] = 1000000
upbd
out <- cut(dataset, breaks = upbd)
tab2_8 <- table(out)

cbind(tab2_8)
barplot(tab2_8,main="Claim sizes",las=2)

# Obtain E(Exp), the fitted claim sizes
eexp <- diff(200*pexp(upbd, rate = 1/mean(dataset)))

cbind(tab2_8,round(eexp))

ntab2_8 <- as.data.frame(cbind(tab2_8,round(eexp)))

# Change the header name ntab2_8
names(ntab2_8)[2] <- "EXP"

#summary of exponential fit using chisq.test function in R
chisq.test(x = tab2_8,
           p = eexp/sum(eexp))

# Manually compute chi-square statistics
sum((tab2_8 - eexp)^2 /eexp)                  #chi-square
1 - pchisq(sum((tab2_8 - eexp)^2 /eexp), 8 )  #Use the same df as 
#given in text. However, this contradict to the chi-squre test using chisq.test where df = 9 is used!

#The following command is not correct
#ks.test(unique(tab2_8), "pexp", rate =  0.0003402187 )
ks.test(dataset, "pexp", rate =  1/mean(dataset) )


##############################################################

# Gamma: X ~ gamma(alpha, lambda)

# MLE method
#\hat\alpha is the value of alpha which maximises the following log-likelihood
#expression:

# We minimize (-1)*la function
la <- function(a)
{-(nsample*a*(log(a) - log(mean(dataset)) -1) + sum(log(dataset))*(a - 1) - nsample*log(gamma(a)))}

##### Plot graph of la to approximate the minimum of la function 

temp <- seq(0.1,5,by=0.1)
plot(temp,la(temp))


# nlm() is Non-Linear Minimization
# p is a starting parameter value for the minimization.
  nlmout <- nlm(la, p = 1,  hessian=TRUE)

# MLE
alpha_hat <- nlmout$estimate               # gives \hat\alpha
lambda_hat <- nlmout$estimate/mean(dataset)  # gives \hat\lambda


# Estimate the cumulative frequencies and then take the difference
# egam <- diff(nsample*pgamma(upbd,alpha_hat ,lambda_hat))

# MME method (However, we will use MLE for the remaining calculation)
alpha_tilde <- (xbar/s)^2
lambda_tilde <- alpha_tilde/xbar

# OR
# mean(dataset)^2/var(dataset)    # gives \tilde\alpha
# mean(dataset)/var(dataset)      # gives \tilde\lambda


# Estimate the cumulative frequencies and then take the difference
egam <- diff(nsample*pgamma(upbd,alpha_hat ,lambda_hat))


# Manually compute chi-square statistics
sum((tab2_8 - egam)^2 /egam)
1 - pchisq( sum((tab2_8 - egam)^2 /egam) , 7 ) # Use the same df as given in the text book. This contradicts with df used in the chi-square test


ks.test(dataset, "pgamma", shape = alpha_hat, rate = lambda_hat )

dist = "gamma"
# Figure 2.20 (left)
hist(dataset,breaks=100,probability = TRUE, xlab = "claim sizes" 
     , ylab = "density", main = paste("Histogram of claim sizes with fitted", dist ,"distribution" ))
curve(dexp(x ,rate = 1/mean(dataset)   ), add = TRUE, col = "blue", lwd = 2)
curve(dgamma(x ,shape = alpha_hat ,rate = lambda_hat), add = TRUE, col = "green", lwd = 2)
legend(300, 0.005, legend=c("Exp", "Gamma"), lty = c(1,1), lwd = 2, col=c("blue","green"))

curve(dgamma(x ,shape = alpha_tilde ,rate = lambda_tilde), add = TRUE, col = "red", lwd = 2)

#lines(density(dataset), col = "red", lwd = 2)
#legend(25000, 2e-4, legend=c("density()", "fitdistr()"), lty = c(1,1), lwd = 2, col=c("red","blue"))


# Figure 2.20 (right)
plot(ecdf(dataset),
     main="Compare ecdf with estimated cdf", xlab="claim sizes", lty=1)
curve(pgamma(x ,shape = alpha_hat ,rate = lambda_hat), add=TRUE, col = "green", lwd = 2)
legend(10000, 0.6, legend=c("ecdf", "estimated cdf"), lty=1 ,col = c("black","green"), lwd = 2)

cbind(tab2_8,round(eexp),round(egam,1))

##############################################################

# Lognormal: X ~ lognormal(mu,sigma^2)

# Check if the logged claim sizes have a normal distribution
logcs <- log(dataset)
summary(logcs)

require(moments)
skewness(logcs)
# The display shows that the logged claim
# sizes have a distribution which may be similar to a normal distribution 
# but with a modest negative skew.

# The tail of the left side of the pdf is longer or fatter than the right side

# Figure 2.22 (left)
hist(logcs,breaks=100,probability = TRUE, xlab = "log(claim size)" 
     , ylab = "density", main = paste("Histogram of log(claim sizes)" ))

qqnorm(logcs, datax = TRUE)
qqline(logcs, datax = TRUE)


# MLE method
# we can find the MLEs of ? and ? directly
# from the logged data as the sample mean and standard deviation of the log(xi)

# Find MLE directly from dataset
mu_hat <- mean(logcs)                             # mu_hat
# Notice -1 in sigma_hat below
sqrt(sum((logcs- mu_hat)^2)/( length(logcs) -1 ))  #sigma_hat
# or
sigma_hat <- sd(logcs)

# Using fitdistr
fitln <- fitdistr(dataset,"lognormal")
p1 <- fitln$estimate[1]
p2 <- fitln$estimate[2]

# MME method
sigma_tilda <- sqrt(log(  var(dataset)/mean(dataset)^2 +1  ))  # gives \tilde\sigma
mu_tilda <- log(mean(dataset)) - sigma_tilda^2/2      # gives \tilde\mu

elognm <- diff(nsample*plnorm(round(upbd), mean(log(dataset)), sd(log(dataset))))

# Replace elognm which are estimates from MME
#elognm <- diff(nsample*plnorm(round(upbd), mu_tilda, sigma_tilda))


cbind(tab2_8,round(eexp),round(egam,1),round(elognm,1))


# Manually compute chi-square statistics
sum((tab2_8 - elognm)^2 /elognm)
1 - pchisq(sum((tab2_8 - elognm)^2 /elognm) , 7 ) # Use the same df as given in the text book. This contradicts with df used in the chi-square test

ks.test(dataset, "plnorm", meanlog = mean(log(dataset)), sdlog = sd(log(dataset))
)

dist = "lognormal"
# Figure 2.23 (left)
hist(dataset,breaks=100,probability = TRUE, xlab = "claim sizes" 
     , ylab = "density", main = paste("Histogram of claim sizes with fitted", dist ,"distribution" ))
curve(dlnorm(x, mean(log(dataset)), sd(log(dataset)))    , add = TRUE, col = "red", lwd = 2)
curve(dgamma(x ,shape = alpha_hat ,rate = lambda_hat), add = TRUE, col = "green", lwd = 2)
legend(200, 0.01, legend=c("Lognormal","Gamma"), lty = 1, lwd = 2, col= c("red","green"))

#lines(density(dataset), col = "red", lwd = 2)
#legend(25000, 2e-4, legend=c("density()", "fitdistr()"), lty = c(1,1), lwd = 2, col=c("red","blue"))


# Figure 2.20 (right)
plot(ecdf(dataset),
     main="Compare ecdf with estimated cdf", xlab="claim sizes", lty=1)
curve(plnorm(x, mean(log(dataset)), sd(log(dataset)))  , add=TRUE, col = "red", lwd = 2)
legend(20000, 0.8, legend=c("ecdf", "estimated cdf"), lty=1 ,col = c("black","green"), lwd = 2)


##############################################################

# Pareto: X ~ Pa(alpha, lambda)

# MME 
alpha_tilde <- 2*var(dataset)/mean(dataset)^2 * 1/(var(dataset)/mean(dataset)^2 - 1) #/tilde/alpha
lambda_tilde <- mean(dataset)*(alpha_tilde -1)

# MLE
# The R function nlm is a minimisation procedure, so the negative of
# the log-likelihood function is declared (as a function of a vector x of length 2)
# as follows:

fp = function(x)
{-(nsample*log(x[1]) + nsample*x[1]*log(x[2]) - (x[1]+1)*sum(log(x[2]+dataset))) }

# Use the MME as the initial parameter for nonlinear maximization
nlmout <- nlm(fp,c(alpha_tilde,lambda_tilde))

alpha_hat <- nlmout$estimate[1]               # gives \hat\alpha
lambda_hat <- nlmout$estimate[2]   # gives \hat\lambda


# Using fitdistr
fitpareto<-fitdist(dataset,"pareto", start = list(shape=1,scale=500))
p1=fitpareto$estimate[1]
p2=fitpareto$estimate[2]



# Estimate the cumulative frequencies and then take the difference
#egam <- diff(nsample*pgamma(upbd,0.6893,0.0002345))
epareto <- diff(nsample*ppareto(upbd,alpha_hat ,lambda_hat))



cbind(tab2_8,round(eexp),round(egam,1),round(elognm,1), round(epareto,1))




# Manually compute chi-square statistics
sum((tab2_8 - round(epareto,1))^2 /round(epareto,1))
1 - pchisq(sum((tab2_8 - epareto)^2 /epareto) , 7 ) # Use the same df as given in the text book. This contradicts with df used in the chi-square test

ks.test(dataset, "ppareto", shape = alpha_hat, scale = lambda_hat)

dist = "pareto"
# Figure 2.24 (left)
hist(dataset,breaks=100,probability = TRUE, xlab = "claim sizes" 
     , ylab = "density", main = paste("Histogram of claim sizes with fitted", dist ,"distribution" ))
curve(dlnorm(x, mean(log(dataset)), sd(log(dataset)))    , add = TRUE, col = "red", lwd = 2)
#curve(dgamma(x ,shape = alpha_hat ,rate = lambda_hat), add = TRUE, col = "green", lwd = 2)
curve(dpareto(x, shape = alpha_hat, scale = lambda_hat), add = TRUE, col = "yellow", lwd = 2)
legend(10000, 0.0015, legend=c("Pareto","lognormal"), lty = 1, lwd = 2, col= c("yellow","red"))

#lines(density(dataset), col = "red", lwd = 2)
#legend(25000, 2e-4, legend=c("density()", "fitdistr()"), lty = c(1,1), lwd = 2, col=c("red","blue"))


# Figure 2.24 (right)
plot(ecdf(dataset),
     main="Compare ecdf with estimated cdf", xlab="claim sizes", lty=1)
curve(ppareto(x, shape = alpha_hat, scale = lambda_hat)  , add=TRUE, col = "yellow", lwd = 2)
legend(20000, 0.8, legend=c("ecdf", "estimated cdf"), lty=1 ,col = c("black","yellow"), lwd = 2)


##############################################################


%\end{MyVerbatim}
\end{verbatim}

\hypertarget{interactive-lecture}{%
\chapter{Interactive Lecture}\label{interactive-lecture}}

Some \emph{significant} applications are demonstrated in this chapter.

\hypertarget{datacamp-light}{%
\section{DataCamp Light}\label{datacamp-light}}

By default, \texttt{tutorial} will convert all R chunks.

eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhIDwtIDJcbmIgPC0gM1xuYSArIGIifQ==

  \bibliography{book.bib,packages.bib}

\end{document}
