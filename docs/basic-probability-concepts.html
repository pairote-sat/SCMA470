<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Basic Probability Concepts | Review of Probability Theory</title>
  <meta name="description" content="Chapter 2 Basic Probability Concepts | Review of Probability Theory" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Basic Probability Concepts | Review of Probability Theory" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Basic Probability Concepts | Review of Probability Theory" />
  
  
  

<meta name="author" content="Pairote Satiracoo" />


<meta name="date" content="2021-08-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="model-fitting.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SCMA470 Risk Analysis and Credibility</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Down the rabbit-hole</a></li>
<li class="chapter" data-level="2" data-path="basic-probability-concepts.html"><a href="basic-probability-concepts.html"><i class="fa fa-check"></i><b>2</b> Basic Probability Concepts</a>
<ul>
<li class="chapter" data-level="2.1" data-path="basic-probability-concepts.html"><a href="basic-probability-concepts.html#expectation"><i class="fa fa-check"></i><b>2.1</b> Expectation</a></li>
<li class="chapter" data-level="2.2" data-path="basic-probability-concepts.html"><a href="basic-probability-concepts.html#variances-of-random-variables"><i class="fa fa-check"></i><b>2.2</b> Variances of Random Variables</a></li>
<li class="chapter" data-level="2.3" data-path="basic-probability-concepts.html"><a href="basic-probability-concepts.html#moments-and-moment-generating-function"><i class="fa fa-check"></i><b>2.3</b> Moments and Moment Generating Function</a></li>
<li class="chapter" data-level="2.4" data-path="basic-probability-concepts.html"><a href="basic-probability-concepts.html#probability-generating-function"><i class="fa fa-check"></i><b>2.4</b> Probability generating function</a></li>
<li class="chapter" data-level="2.5" data-path="basic-probability-concepts.html"><a href="basic-probability-concepts.html#multivariate-distributions"><i class="fa fa-check"></i><b>2.5</b> Multivariate Distributions</a></li>
<li class="chapter" data-level="2.6" data-path="basic-probability-concepts.html"><a href="basic-probability-concepts.html#independent-random-variables"><i class="fa fa-check"></i><b>2.6</b> Independent random variables</a></li>
<li class="chapter" data-level="2.7" data-path="basic-probability-concepts.html"><a href="basic-probability-concepts.html#conditional-distributions"><i class="fa fa-check"></i><b>2.7</b> Conditional Distributions</a></li>
<li class="chapter" data-level="2.8" data-path="basic-probability-concepts.html"><a href="basic-probability-concepts.html#covariance"><i class="fa fa-check"></i><b>2.8</b> Covariance</a></li>
<li class="chapter" data-level="2.9" data-path="basic-probability-concepts.html"><a href="basic-probability-concepts.html#correlation"><i class="fa fa-check"></i><b>2.9</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="model-fitting.html"><a href="model-fitting.html"><i class="fa fa-check"></i><b>3</b> Model Fitting</a>
<ul>
<li class="chapter" data-level="3.1" data-path="model-fitting.html"><a href="model-fitting.html#the-method-of-moments"><i class="fa fa-check"></i><b>3.1</b> The method of moments</a></li>
<li class="chapter" data-level="3.2" data-path="model-fitting.html"><a href="model-fitting.html#the-method-of-maximum-likelihood"><i class="fa fa-check"></i><b>3.2</b> The method of maximum likelihood</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html"><i class="fa fa-check"></i><b>4</b> Goodness of fit tests</a>
<ul>
<li class="chapter" data-level="4.1" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html#the-pearson-chi-square-goodness-of-fit-criterion"><i class="fa fa-check"></i><b>4.1</b> the Pearson chi-square goodness-of-fit criterion</a></li>
<li class="chapter" data-level="4.2" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html#kolmogorov-smirnov-k-s-test."><i class="fa fa-check"></i><b>4.2</b> Kolmogorov-Smirnov (K-S) test.</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="a-caucus-race-and-a-long-tale.html"><a href="a-caucus-race-and-a-long-tale.html"><i class="fa fa-check"></i><b>5</b> A caucus-race and a long tale</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><strong>Review of Probability Theory</strong></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basic-probability-concepts" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Basic Probability Concepts</h1>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 2.1  </strong></span><strong>Definition 1</strong>. <em>Let <span class="math inline">\(S\)</span> be the sample space of an experiment. A
real-valued function <span class="math inline">\(X : S \rightarrow \mathbb{R}\)</span> is called a <strong>random
variable</strong> of the experiment if, for each interval
<span class="math inline">\(I \subset \mathbb{R}, \, \{s : X(s) \in I \}\)</span> is an event. </em></p>
</div>
<p>Random variables are often used for the calculation of the probabilities
of events. The real-valued function <span class="math inline">\(P(X \le t)\)</span> characterizes <span class="math inline">\(X\)</span>, it
tells us almost everything about <span class="math inline">\(X\)</span>. This function is called the
<strong>cumulative distribution function</strong> of <span class="math inline">\(X\)</span>. The cumulative distribution
function describes how the probabilities accumulate.</p>
<div class="definition">
<p><span id="def:unlabeled-div-2" class="definition"><strong>Definition 2.2  </strong></span><strong>Definition 2</strong>. <em>If <span class="math inline">\(X\)</span> is a random variable, then the function <span class="math inline">\(F\)</span>
defined on <span class="math inline">\(\mathbb{R}\)</span> by <span class="math display">\[F(x) = P(X \le x)\]</span> is called the
<strong>cumulative distribution function</strong> or simply <strong>distribution function
(c.d.f)</strong> of <span class="math inline">\(X\)</span>.</em></p>
</div>
<p>Functions that define the probability measure for discrete and
continuous random variables are the probability mass function and the
probability density function.</p>
<div class="definition">
<p><span id="def:unlabeled-div-3" class="definition"><strong>Definition 2.3  </strong></span><strong>Definition 3</strong>. <em>Suppose <span class="math inline">\(X\)</span> is a discrete random variable. Then the
function <span class="math display">\[f(x) = P(X = x)\]</span> that is defined for each <span class="math inline">\(x\)</span> in the range
of <span class="math inline">\(X\)</span> is called the <strong>probability mass function</strong> (p.m.f) of a random
variable <span class="math inline">\(X\)</span>.</em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 2.4  </strong></span><strong>Definition 4</strong>. <em>Suppose <span class="math inline">\(X\)</span> is a continuous random variable with
c.d.f <span class="math inline">\(F\)</span> and there exists a nonnegative, integrable function <span class="math inline">\(f\)</span>,
<span class="math inline">\(f: \mathbb{R} \rightarrow [0, \infty)\)</span> such that
<span class="math display">\[F(x) = \int_{-\infty}^x f(y)\, dy\]</span> Then the function <span class="math inline">\(f\)</span> is called
the <strong>probability density function</strong> (p.d.f) of a random variable <span class="math inline">\(X\)</span>.</em></p>
</div>
<div id="expectation" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Expectation</h2>
<div class="definition">
<p><span id="def:unlabeled-div-5" class="definition"><strong>Definition 2.5  </strong></span><strong>Definition 5</strong>. <em>The <strong>expected value</strong> of a discrete random variable
<span class="math inline">\(X\)</span> with the set of possible values <span class="math inline">\(A\)</span> and probability mass function
<span class="math inline">\(f(x)\)</span> is defined by <span class="math display">\[\mathrm{E}(X) = \sum_{x \in A} x f(x)\]</span></em></p>
</div>
<p>The <strong>expected value</strong> of a random variable <span class="math inline">\(X\)</span> is also called the mean,
or the mathematical expectation, or simply the expectation of <span class="math inline">\(X\)</span>. It is
also occasionally denoted by <span class="math inline">\(\mathrm{E}[X]\)</span>, <span class="math inline">\(\mu_X\)</span>, or <span class="math inline">\(\mu\)</span>.</p>
<p>Note that if each value <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span> is weighted by <span class="math inline">\(f(x) = P(X = x)\)</span>,
then <span class="math inline">\(\displaystyle \sum_{x \in A} x f(x)\)</span> is nothing but the weighted
average of <span class="math inline">\(X\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-6" class="theorem"><strong>Theorem 2.1  </strong></span><strong>Theorem 1</strong>. <em>Let <span class="math inline">\(X\)</span> be a discrete random variable with set of
possible values <span class="math inline">\(A\)</span> and probability mass function <span class="math inline">\(f(x)\)</span>, and let <span class="math inline">\(g\)</span> be
a real-valued function. Then <span class="math inline">\(g(X)\)</span> is a random variable with
<span class="math display">\[\mathrm{E}[g(X)] = \sum_{x \in A} g(x) f(x)\]</span> </em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-7" class="definition"><strong>Definition 2.6  </strong></span><strong>Definition 6</strong>. <em>If <span class="math inline">\(X\)</span> is a continuous random variable with
probability density function <span class="math inline">\(f\)</span> , the <strong>expected value</strong> of <span class="math inline">\(X\)</span> is
defined by <span class="math display">\[\mathrm{E}(X) = \int_{-\infty}^\infty x f(x)\, dx\]</span> </em></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-8" class="theorem"><strong>Theorem 2.2  </strong></span><strong>Theorem 2</strong>. * Let <span class="math inline">\(X\)</span> be a continuous random variable with
probability density function <span class="math inline">\(f (x)\)</span>; then for any function
<span class="math inline">\(h: \mathbb{R} \rightarrow \mathbb{R}\)</span>,
<span class="math display">\[\mathrm{E}[h(X)] = \int_{-\infty}^\infty h(x)\, f(x)\, dx\]</span> *</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-9" class="theorem"><strong>Theorem 2.3  </strong></span><strong>Theorem 3</strong>. <em>Let <span class="math inline">\(X\)</span> be a random variable. Let
<span class="math inline">\(h_1, h_2, . . . , h_n\)</span> be real-valued functions, and
<span class="math inline">\(a_1, a_2, \ldots, a_n\)</span> be real numbers. Then
<span class="math display">\[\mathrm{E}[a_1 h_1(X) + a_2 h_2(X) + \cdots + a_n h_n(X)] = a_1 \mathrm{E}[h_1(X)] + a_2 \mathrm{E}[h_2(X)] + \ldots +
    a_n \mathrm{E}[h_n(X)]\]</span></em></p>
</div>
<p>Moreover, if <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, then
<span class="math display">\[\mathrm{E}(aX +b) = a\mathrm{E}(x) + b\]</span></p>
</div>
<div id="variances-of-random-variables" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Variances of Random Variables</h2>
<div class="definition">
<p><span id="def:unlabeled-div-10" class="definition"><strong>Definition 2.7  </strong></span><strong>Definition 7</strong>. <em>Let <span class="math inline">\(X\)</span> be a discrete random variable with a set of
possible values <span class="math inline">\(A\)</span>, probability mass function <span class="math inline">\(f(x)\)</span>, and
<span class="math inline">\(\mathrm{E}(X) = \mu\)</span>. then <span class="math inline">\(\mathrm{Var}(X)\)</span> and <span class="math inline">\(\sigma_X\)</span>, called the
<strong>variance</strong> and <strong>standard deviation</strong> of <span class="math inline">\(X\)</span>, respectively, are
defined by
<span class="math display">\[\mathrm{Var}(X) = \mathrm{E}[(X- \mu)^2] = \sum_{x \in A} (x - \mu)^2 f(x),\]</span>
<span class="math display">\[\sigma_X = \sqrt{\mathrm{E}[(X- \mu)^2]}\]</span></em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-11" class="definition"><strong>Definition 2.8  </strong></span><strong>Definition 8</strong>. <em>If <span class="math inline">\(X\)</span> is a continuous random variable with
<span class="math inline">\(\mathrm{E}(X) = \mu\)</span>, then <span class="math inline">\(\mathrm{Var}(X)\)</span> and <span class="math inline">\(\sigma_X\)</span>, called the
<strong>variance</strong> and <strong>standard deviation</strong> of <span class="math inline">\(X\)</span>, respectively, are
defined by
<span class="math display">\[\mathrm{Var}(X) = \mathrm{E}[(X- \mu)^2] =  \int_{-\infty}^\infty (x - \mu)^2\, f(x)\, dx ,\]</span>
<span class="math display">\[\sigma_X = \sqrt{\mathrm{E}[(X- \mu)^2]}\]</span></em></p>
</div>
<p>We have the following important relations
<span class="math display">\[\mathrm{Var}(x) = \mathrm{E}(X^2) - (\mathrm{E}(x))^2 ,\]</span>
<span class="math display">\[\mathrm{Var}(aX + b) = a^2\ Var(X), \quad   \sigma_{aX + b}= |a|\sigma_X\]</span>
where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants.</p>
</div>
<div id="moments-and-moment-generating-function" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Moments and Moment Generating Function</h2>
<div class="definition">
<p><span id="def:unlabeled-div-12" class="definition"><strong>Definition 2.9  </strong></span><strong>Definition 9</strong>. <em>For <span class="math inline">\(r &gt; 0\)</span>, the <span class="math inline">\(r\)</span>th moment of <span class="math inline">\(X\)</span> (the <span class="math inline">\(r\)</span>th
moment about the origin) is <span class="math inline">\(\mathrm{E}[X^r]\)</span>, when it is defined. The
<span class="math inline">\(r\)</span>th central moment of a random variable <span class="math inline">\(X\)</span> (the <span class="math inline">\(r\)</span>th moment about
the mean) is <span class="math inline">\(\mathrm{E}[(X - \mathrm{E}[X])^r].\)</span> </em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-13" class="definition"><strong>Definition 2.10  </strong></span><strong>Definition 10</strong>. <em>The skewness of <span class="math inline">\(X\)</span> is defined to be the third
central moment, <span class="math display">\[\mathrm{E}[(X - \mathrm{E}[X])^3],\]</span> and the
coefficient of skewness to be given by
<span class="math display">\[\frac{\mathrm{E}[(X - \mathrm{E}[X])^3]}{(\mathrm{Var}[X])^{3/2}}.\]</span> </em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-14" class="definition"><strong>Definition 2.11  </strong></span><strong>Definition 11</strong>. <em>The coefficient of kurtosis of <span class="math inline">\(X\)</span> is defined by
<span class="math display">\[\frac{\mathrm{E}[(X - \mathrm{E}[X])^4]}{(\mathrm{Var}[X])^{4/2}}.\]</span> </em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-15" class="definition"><strong>Definition 2.12  </strong></span><strong>Definition 12</strong>. * The moment generating function (mgf) of a random
variable <span class="math inline">\(X\)</span> is defined to be <span class="math display">\[M_X(t) = E[e^{tX}],\]</span> if the expectation
exists. *</p>
</div>
<p><strong>Note</strong> The moment generating function of <span class="math inline">\(X\)</span> may not defined (may not
be finite) for all <span class="math inline">\(t\)</span> in <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>If <span class="math inline">\(M_X(t)\)</span> is finite for <span class="math inline">\(|t| &lt; h\)</span> for some <span class="math inline">\(h &gt; 0\)</span>, then, for any
<span class="math inline">\(k = 1, 2, \ldots,\)</span> the function <span class="math inline">\(M_X(t)\)</span> is k-times differentiable at
<span class="math inline">\(t = 0\)</span>, with <span class="math display">\[M^{(k)}_X (0) = \mathrm{E}[X^k],\]</span> with
<span class="math inline">\(\mathrm{E}[|X|^k]\)</span> finite. We can obtain the moments by succesive
differentiation of <span class="math inline">\(M_X(t)\)</span> and letting <span class="math inline">\(t = 0\)</span>.</p>
</div>
<div id="probability-generating-function" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Probability generating function</h2>
<div class="definition">
<p><span id="def:unlabeled-div-16" class="definition"><strong>Definition 2.13  </strong></span><strong>Definition 13</strong>. * For a counting variable <span class="math inline">\(N\)</span> (a variable which
assumes some or all of the values <span class="math inline">\(0, 1, 2, \ldots,\)</span> but no others), The
probability generating function of <span class="math inline">\(N\)</span> is <span class="math display">\[G_N(t) = E[t^N],\]</span> for those
<span class="math inline">\(t\)</span> in <span class="math inline">\(\mathbb{R}\)</span> for which the series converges absolutely. *</p>
</div>
<p>Let <span class="math inline">\(p_k = P(N = k)\)</span>. Then
<span class="math display">\[G_N(t) = E[t^N] = \sum_{k=0}^\infty t^k p_k.\]</span> It can be shown that if
<span class="math inline">\(E[N] &lt; \infty\)</span> then <span class="math display">\[\mathrm{E}[N] = G&#39;_N(1),\]</span> and if
<span class="math inline">\(E[N^2] &lt; \infty\)</span> then
<span class="math display">\[\mathrm{Var}[N] = G&#39;&#39;_N(1) + G&#39;_N(1) - (G&#39;_N(1))^2.\]</span> Moreover, when
both pgf and mgf of <span class="math inline">\(N\)</span> are defined, we have
<span class="math display">\[G_N(t) = M_N(\log(t)) \quad \text{ and } M_N(t) = G_N(e^t).\]</span></p>
</div>
<div id="multivariate-distributions" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Multivariate Distributions</h2>
<p>When <span class="math inline">\(X_1,X_2,\ldots ,X_n\)</span> be random variables defined on the same
sample space, a multivariate probability density function or probability
mass function<br />
<span class="math inline">\(f(x_1, x_2, \ldots x_n)\)</span> can be defined. The following definitions can
be extended to more than two random variables and the case of discrete
random variables.</p>
<div class="definition">
<p><span id="def:unlabeled-div-17" class="definition"><strong>Definition 2.14  </strong></span><strong>Definition 14</strong>. <em>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, defined on the
same sample space, have a continuous joint distribution if there exists
a nonnegative function of two variables, <span class="math inline">\(f(x, y)\)</span> on
<span class="math inline">\(\mathbb{R} \times \mathbb{R}\)</span> , such that for any region <span class="math inline">\(R\)</span> in the
<span class="math inline">\(xy\)</span>-plane that can be formed from rectangles by a countable number of
set operations, <span class="math display">\[P((X, Y)  \in R) = \iint_R f(x,y) \, dx\, dy\]</span> </em></p>
</div>
<p>The function <span class="math inline">\(f (x, y)\)</span> is called the <strong>joint probability density
function</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have joint probability density function <span class="math inline">\(f (x, y)\)</span>. Let
<span class="math inline">\(f_Y\)</span> be the probability density function of <span class="math inline">\(Y\)</span> . To find <span class="math inline">\(f_Y\)</span> in
terms of <span class="math inline">\(f\)</span> , note that, on the one hand, for any subset <span class="math inline">\(B\)</span> of <span class="math inline">\(R\)</span>,
<span class="math display">\[P(Y \in B) = \int_B f_Y(y) \, dy,\]</span> and on the other hand, we also
have
<span class="math display">\[P(Y \in B) = P(X \in (-\infty, \infty), Y \in B) = \int_B \left(    \int_{-\infty}^\infty f(x,y)\, dx  \right) \, dy.\]</span></p>
<p>We have <span class="math display">\[\label{mar1}  f_Y(y) =   \int_{-\infty}^\infty f(x,y)\, dx\]</span>
and <span class="math display">\[\label{mar2}  f_X(x) =   \int_{-\infty}^\infty f(x,y)\, dy\]</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-18" class="definition"><strong>Definition 2.15  </strong></span><strong>Definition 15</strong>. <em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have joint probability density
function <span class="math inline">\(f (x, y)\)</span>; then the functions <span class="math inline">\(f_X\)</span> and <span class="math inline">\(f_Y\)</span> in
<a href="#mar2" reference-type="eqref" reference="mar2"><span class="math display">\[mar2\]</span></a> and
<a href="#mar1" reference-type="eqref" reference="mar1"><span class="math display">\[mar1\]</span></a> are called,
respectively, the <strong>marginal probability density functions</strong> of <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span> .</em></p>
</div>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables (discrete, continuous, or
mixed). The <strong>joint probability distribution function</strong>, or <strong>joint
cumulative probability distribution function</strong>, or simply the joint
distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, is defined by
<span class="math display">\[F(t, u) = P(X \le t, Y \le u)\]</span> for all <span class="math inline">\(t, u \in (-\infty, \infty)\)</span>.</p>
<p>The marginal probability distribution function of <span class="math inline">\(X\)</span>, <span class="math inline">\(F_X\)</span>, can be
found from <span class="math inline">\(F\)</span> as follows:
<span class="math display">\[F_X(t) = \lim_{n \rightarrow \infty} F(t,u)  = F(t, \infty)\]</span> and
<span class="math display">\[F_Y(u) = \lim_{n \rightarrow \infty}F(t,u)  = F( \infty, u)\]</span></p>
<p>The following relationship between <span class="math inline">\(f(x,y)\)</span> and <span class="math inline">\(F(t,u)\)</span> is as follows:
<span class="math display">\[F(t,u) = \int_{-\infty}^{u}\int_{-\infty}^{t} f(x,y)\, dx\, dy.\]</span></p>
<p>We also have
<span class="math display">\[\mathrm{E}(X) =   \int_{-\infty}^\infty x f_X(x)\, dx , \quad \mathrm{E}(Y) =   \int_{-\infty}^\infty y f_Y(y)\, dy\]</span></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-19" class="theorem"><strong>Theorem 2.4  </strong></span><strong>Theorem 4</strong>. <em>Let <span class="math inline">\(f (x, y)\)</span> be the joint probability density function
of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. If <span class="math inline">\(h\)</span> is a function of two variables
from <span class="math inline">\(\mathbb{R}^2\)</span> to <span class="math inline">\(\mathbb{R}\)</span>, then <span class="math inline">\(h(X, Y )\)</span> is a random
variable with the expected value given by
<span class="math display">\[\mathrm{E}[h(X,Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} h(x,y) \, f(x,y)\, dx\, dy\]</span>
provided that the integral is absolutely convergent.</em></p>
</div>
<p>As a consequence of the above theorem, for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,
<span class="math display">\[\mathrm{E}(X + Y) = \mathrm{E}(X) + \mathrm{E}(Y)\]</span></p>
</div>
<div id="independent-random-variables" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Independent random variables</h2>
<div class="definition">
<p><span id="def:unlabeled-div-20" class="definition"><strong>Definition 2.16  </strong></span><strong>Definition 16</strong>. <em>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are called
independent if, for arbitrary subsets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> of real numbers, the
events <span class="math inline">\(\{X \in A\}\)</span> and <span class="math inline">\(\{Y \in B\}\)</span> are <strong>independent</strong>, that is, if
<span class="math display">\[P(X \in A, Y \in B) = P(X \in A) P(Y \in B).\]</span></em></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-21" class="theorem"><strong>Theorem 2.5  </strong></span><strong>Theorem 5</strong>. <em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables defined on the
same sample space. If <span class="math inline">\(F\)</span> is the joint probability distribution function
of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if and only if for all
real numbers <span class="math inline">\(t\)</span> and <span class="math inline">\(u\)</span>, <span class="math display">\[F(t,u)  = F_X(t) F_Y(u).\]</span></em></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-22" class="theorem"><strong>Theorem 2.6  </strong></span><strong>Theorem 6</strong>. <em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be jointly continuous random variables
with joint probability density function <span class="math inline">\(f (x, y)\)</span>. Then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are
independent if and only if <span class="math display">\[f (x, y) = f_X(x) f_Y (y).\]</span></em></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-23" class="theorem"><strong>Theorem 2.7  </strong></span><strong>Theorem 7</strong>. <em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables and
<span class="math inline">\(g : \mathbb{R} \rightarrow\mathbb{R}\)</span> and
<span class="math inline">\(h : \mathbb{R} \rightarrow\mathbb{R}\)</span> be real-valued functions; then
<span class="math inline">\(g(X)\)</span> and <span class="math inline">\(h(Y )\)</span> are also independent random variables.</em></p>
</div>
<p>As a consequence of the above theorem, we obtain</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-24" class="theorem"><strong>Theorem 2.8  </strong></span><strong>Theorem 8</strong>. <em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables. Then
for all real-value functions <span class="math inline">\(g : \mathbb{R} \rightarrow\mathbb{R}\)</span> and
<span class="math inline">\(h : \mathbb{R} \rightarrow\mathbb{R}\)</span>,
<span class="math display">\[\mathrm{E}[g(X)h(Y)] = \mathrm{E}[g(X)]\mathrm{E}[h(Y)]\]</span> </em></p>
</div>
</div>
<div id="conditional-distributions" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Conditional Distributions</h2>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two continuous random variables with the joint
probability density function <span class="math inline">\(f (x, y)\)</span>. Note that the case of discrete
random variables can be considered in the same way. When no information
is given about the value of <span class="math inline">\(Y\)</span>, the marginal probability density
function of <span class="math inline">\(X\)</span>, <span class="math inline">\(f_X(x)\)</span> is used to calculate the probabilities of
events concerning <span class="math inline">\(X\)</span>. However, when the value of <span class="math inline">\(Y\)</span> is known, to find
such probabilities, <span class="math inline">\(f_{X|Y} (x|y)\)</span>, the conditional probability density
function of <span class="math inline">\(X\)</span> given that <span class="math inline">\(Y = y\)</span> is used and is defined as follows:
<span class="math display">\[f_{X|Y} (x|y)  = \frac{f(x,y)}{f_Y(y)}\]</span> provided that <span class="math inline">\(f_Y (y) &gt; 0\)</span>.
Note also that the conditional probability density function of <span class="math inline">\(X\)</span> given
that <span class="math inline">\(Y = y\)</span> is itseof a probability density function, i.e.
<span class="math display">\[\int_{-\infty}^\infty f_{X|Y}(x|y)\, dx  =  1.\]</span></p>
<p>Note that the conditional probability distribution function of <span class="math inline">\(X\)</span> given
that <span class="math inline">\(Y = y\)</span>, the conditional expectation of <span class="math inline">\(X\)</span> given that <span class="math inline">\(Y = y\)</span> can
be as follows:
<span class="math display">\[F_{Y|X}(x|y) = P(X \le x | Y = y) = \int_ {-\infty}^x f_{X|Y}(t|y) \, dt\]</span>
and
<span class="math display">\[\mathrm{E}(X|Y = y) =  \int_{-\infty}^{\infty} x  f_{X|Y}(x|y) \, dx,\]</span>
where <span class="math inline">\(f_Y(y) &gt; 0\)</span>.</p>
<p>Note that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(f_{X|Y}\)</span> coincides with
<span class="math inline">\(f_X\)</span> because
<span class="math display">\[f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)} =\frac{f_X(x)f_Y(y)}{f_Y(y)} = f_X(x).\]</span></p>
</div>
<div id="covariance" class="section level2" number="2.8">
<h2><span class="header-section-number">2.8</span> Covariance</h2>
<p>The notion of the variance of a random variable <span class="math inline">\(X\)</span>,
<span class="math inline">\(\mathrm{Var}(X) = \mathrm{E}[ ( X - \mathrm{E}(X))^2]\)</span> measures the
average magnitude of the fluctuations of the random variable <span class="math inline">\(X\)</span> from
its expectation, <span class="math inline">\(\mathrm{E}(X)\)</span>. This quantity measures the dispersion,
or spread, of the distribution of <span class="math inline">\(X\)</span> about its expectation. Now suppose
that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two jointly distributed random variables.
Covariance is a measure of how much two random variables vary together.</p>
<p>Let us calculuate <span class="math inline">\(\mathrm{Var}(aX + bY)\)</span> the joint spread, or
dispersion, of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> along the <span class="math inline">\((ax + by)\)</span>-direction for arbitrary
real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:
<span class="math display">\[\mathrm{Var}(aX + bY) = a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) + 2 a b \mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))].\]</span>
However, <span class="math inline">\(\mathrm{Var}(X)\)</span> and <span class="math inline">\(\mathrm{Var}(Y )\)</span> determine the
dispersions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> independently; therefore,
<span class="math inline">\(\mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))]\)</span> is the quantity
that gives information about the joint spread, or dispersion, <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span> .</p>
<div class="definition">
<p><span id="def:unlabeled-div-25" class="definition"><strong>Definition 2.17  </strong></span><strong>Definition 17</strong>. <em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be jointly distributed random
variables; then the <strong>covariance</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined by
<span class="math display">\[\mathrm{Cov}(X,Y) =  \mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))].\]</span></em></p>
</div>
<p>Note that for random variables <span class="math inline">\(X, Y\)</span> and <span class="math inline">\(Z\)</span>, and <span class="math inline">\(ab &gt; 0\)</span>, then the
joint dispersion of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> along the <span class="math inline">\((ax + by)\)</span>-direction is
greater than the joint dispersion of <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> along the
<span class="math inline">\((ax + bz)\)</span>-direction if and only if
<span class="math inline">\(\mathrm{Cov}(X, Y) &gt; \mathrm{Cov}(X,Z).\)</span></p>
<p>Note that <span class="math display">\[\mathrm{Cov}(X, X) = \mathrm{Var}(X).\]</span> Moreover,
<span class="math display">\[\mathrm{Cov}(X,Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y).\]</span></p>
<p>Properties of covariance are as follows: for arbitrary real numbers
<span class="math inline">\(a, b, c, d\)</span> and random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,
<span class="math display">\[\mathrm{Var}(aX + bY) = a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) + 2 a b \mathrm{Cov}(X,Y).\]</span>
<span class="math display">\[\mathrm{Cov}(aX + b, cY + d) = acCov(X, Y)\]</span> For random variables
<span class="math inline">\(X_1, X_2, . . . , X_n\)</span> and <span class="math inline">\(Y_1, Y_2, . . . , Y_m\)</span>,
<span class="math display">\[\mathrm{Cov}(\sum_{i=1}^n a_i X_i, \sum_{j=1}^m b_j Y_j) = \sum_{i=1}^n\sum_{j=1}^m a_i\,b_j\, \mathrm{Cov}(X_i,Y_j).\]</span></p>
<p>If <span class="math inline">\(\mathrm{Cov}(X, Y) &gt; 0\)</span>, we say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are positively
correlated. If <span class="math inline">\(\mathrm{Cov}(X, Y) &lt; 0\)</span>, we say that they are negatively
correlated. If <span class="math inline">\(\mathrm{Cov}(X, Y) = 0\)</span>, we say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are
uncorrelated.</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math display">\[\mathrm{Cov}(X,Y) = 0.\]</span> However,
the converse of this is not true; that is, two dependent random
variables might be uncorrelated.</p>
</div>
<div id="correlation" class="section level2" number="2.9">
<h2><span class="header-section-number">2.9</span> Correlation</h2>
<p>A large covariance can mean a strong relationship between variables.
However, we cannot compare variances over data sets with different
scales. A weak covariance in one data set may be a strong one in a
different data set with different scales. The problem can be fixed by
dividing the covariance by the standard deviation to get the correlation
coefficient.</p>
<div class="definition">
<p><span id="def:unlabeled-div-26" class="definition"><strong>Definition 2.18  </strong></span><strong>Definition 18</strong>. <em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables with
<span class="math inline">\(0&lt; \sigma^2_X, \sigma^2_Y &lt; \infty\)</span>. The covariance between the
standardized <span class="math inline">\(X\)</span> and the standardized <span class="math inline">\(Y\)</span> is called the correlation
coefficient between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and is denoted <span class="math inline">\(\rho = \rho(X,Y)\)</span>,
<span class="math display">\[\rho(X,Y) = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}.\]</span> </em></p>
</div>
<p>Note that</p>
<ul>
<li><p><span class="math inline">\(\rho(X, Y ) &gt; 0\)</span> if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are positively
correlated;</p></li>
<li><p><span class="math inline">\(\rho(X, Y ) &lt; 0\)</span> if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are negatively
correlated; and</p></li>
<li><p><span class="math inline">\(\rho(X, Y ) = 0\)</span> if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated.</p></li>
<li><p><span class="math inline">\(\rho(X, Y )\)</span> roughly measures the amount and the sign of linear
relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<p>In the case of perfect linear relationship, we have
<span class="math inline">\(\rho(X, Y ) = \pm1\)</span>. A correlation of 0, i.e. <span class="math inline">\(\rho(X, Y ) = 0\)</span> does
not mean zero relationship between two variables; rather, it means zero
linear relationship.</p>
<p>Some importants properties of correlation are
<span class="math display">\[-1 \le \rho(X, Y ) \le 1\]</span>
<span class="math display">\[\rho(a X + b, cY +d) = \text{sign}(ac) \rho(X, Y )\]</span></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-fitting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pairote-sat/SCMA470/edit/master/02-tears.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/pairote-sat/SCMA470/blob/master/02-tears.Rmd",
"text": null
},
"download": ["SCMA470Bookdownproj.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
