<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>SCMA470 Risk Analysis and Credibility</title>
  <meta name="description" content="SCMA470 Risk Analysis and Credibility" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="SCMA470 Risk Analysis and Credibility" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="SCMA470 Risk Analysis and Credibility" />
  
  
  

<meta name="author" content="Pairote Satiracoo" />


<meta name="date" content="2021-09-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="loss-distributions.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SCMA470 Risk Analysis and Credibility</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Basic Probability Concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.1</b> Random Variables</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#r-functions-for-probability-distributions"><i class="fa fa-check"></i><b>1.1.1</b> R Functions for Probability Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#expectation"><i class="fa fa-check"></i><b>1.2</b> Expectation</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#variances-of-random-variables"><i class="fa fa-check"></i><b>1.3</b> Variances of Random Variables</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#moments-and-moment-generating-function"><i class="fa fa-check"></i><b>1.4</b> Moments and Moment Generating Function</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#probability-generating-function"><i class="fa fa-check"></i><b>1.5</b> Probability generating function</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#multivariate-distributions"><i class="fa fa-check"></i><b>1.6</b> Multivariate Distributions</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#independent-random-variables"><i class="fa fa-check"></i><b>1.7</b> Independent random variables</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#conditional-distributions"><i class="fa fa-check"></i><b>1.8</b> Conditional Distributions</a></li>
<li class="chapter" data-level="1.9" data-path="index.html"><a href="index.html#covariance"><i class="fa fa-check"></i><b>1.9</b> Covariance</a></li>
<li class="chapter" data-level="1.10" data-path="index.html"><a href="index.html#correlation"><i class="fa fa-check"></i><b>1.10</b> Correlation</a></li>
<li class="chapter" data-level="1.11" data-path="index.html"><a href="index.html#model-fitting"><i class="fa fa-check"></i><b>1.11</b> Model Fitting</a></li>
<li class="chapter" data-level="1.12" data-path="index.html"><a href="index.html#the-method-of-moments"><i class="fa fa-check"></i><b>1.12</b> The method of moments</a></li>
<li class="chapter" data-level="1.13" data-path="index.html"><a href="index.html#the-method-of-maximum-likelihood"><i class="fa fa-check"></i><b>1.13</b> The method of maximum likelihood</a></li>
<li class="chapter" data-level="1.14" data-path="index.html"><a href="index.html#goodness-of-fit-tests"><i class="fa fa-check"></i><b>1.14</b> Goodness of fit tests</a></li>
<li class="chapter" data-level="1.15" data-path="index.html"><a href="index.html#the-pearson-chi-square-goodness-of-fit-criterion"><i class="fa fa-check"></i><b>1.15</b> the Pearson chi-square goodness-of-fit criterion</a></li>
<li class="chapter" data-level="1.16" data-path="index.html"><a href="index.html#kolmogorov-smirnov-k-s-test."><i class="fa fa-check"></i><b>1.16</b> Kolmogorov-Smirnov (K-S) test.</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="loss-distributions.html"><a href="loss-distributions.html"><i class="fa fa-check"></i><b>2</b> Loss distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="loss-distributions.html"><a href="loss-distributions.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="loss-distributions.html"><a href="loss-distributions.html#exponential-distribution"><i class="fa fa-check"></i><b>2.2</b> Exponential Distribution</a></li>
<li class="chapter" data-level="2.3" data-path="loss-distributions.html"><a href="loss-distributions.html#gamma-distribution"><i class="fa fa-check"></i><b>2.3</b> Gamma distribution</a></li>
<li class="chapter" data-level="2.4" data-path="loss-distributions.html"><a href="loss-distributions.html#lognormal-distribution"><i class="fa fa-check"></i><b>2.4</b> Lognormal distribution</a></li>
<li class="chapter" data-level="2.5" data-path="loss-distributions.html"><a href="loss-distributions.html#pareto-distribution"><i class="fa fa-check"></i><b>2.5</b> Pareto distribution</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="deductibles-and-reinsurance.html"><a href="deductibles-and-reinsurance.html"><i class="fa fa-check"></i><b>3</b> Deductibles and reinsurance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="deductibles-and-reinsurance.html"><a href="deductibles-and-reinsurance.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="deductibles-and-reinsurance.html"><a href="deductibles-and-reinsurance.html#deductibles"><i class="fa fa-check"></i><b>3.2</b> Deductibles</a></li>
<li class="chapter" data-level="3.3" data-path="deductibles-and-reinsurance.html"><a href="deductibles-and-reinsurance.html#reinsurance"><i class="fa fa-check"></i><b>3.3</b> Reinsurance</a></li>
<li class="chapter" data-level="3.4" data-path="deductibles-and-reinsurance.html"><a href="deductibles-and-reinsurance.html#excess-of-loss-reinsurance"><i class="fa fa-check"></i><b>3.4</b> Excess of loss reinsurance</a></li>
<li class="chapter" data-level="3.5" data-path="deductibles-and-reinsurance.html"><a href="deductibles-and-reinsurance.html#mixed-distributions"><i class="fa fa-check"></i><b>3.5</b> Mixed distributions</a></li>
<li class="chapter" data-level="3.6" data-path="deductibles-and-reinsurance.html"><a href="deductibles-and-reinsurance.html#the-distribution-of-reinsurance-claims"><i class="fa fa-check"></i><b>3.6</b> The distribution of reinsurance claims</a></li>
<li class="chapter" data-level="3.7" data-path="deductibles-and-reinsurance.html"><a href="deductibles-and-reinsurance.html#proportional-reinsurance"><i class="fa fa-check"></i><b>3.7</b> Proportional reinsurance</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="collective-risk-model.html"><a href="collective-risk-model.html"><i class="fa fa-check"></i><b>4</b> Collective Risk Model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="collective-risk-model.html"><a href="collective-risk-model.html#conditional-expectation-and-variance-formulas"><i class="fa fa-check"></i><b>4.1</b> Conditional expectation and variance formulas</a></li>
<li class="chapter" data-level="4.2" data-path="collective-risk-model.html"><a href="collective-risk-model.html#sectionCompoundDistribution"><i class="fa fa-check"></i><b>4.2</b> The moments of a compound distribution <span class="math inline">\(S\)</span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="collective-risk-model.html"><a href="collective-risk-model.html#the-mean-of-s"><i class="fa fa-check"></i><b>4.2.1</b> The mean of <span class="math inline">\(S\)</span></a></li>
<li class="chapter" data-level="4.2.2" data-path="collective-risk-model.html"><a href="collective-risk-model.html#the-variance-of-s"><i class="fa fa-check"></i><b>4.2.2</b> The variance of <span class="math inline">\(S\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="collective-risk-model.html"><a href="collective-risk-model.html#special-compound-distributions"><i class="fa fa-check"></i><b>4.3</b> Special compound distributions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="collective-risk-model.html"><a href="collective-risk-model.html#compound-poisson-distributions"><i class="fa fa-check"></i><b>4.3.1</b> Compound Poisson distributions</a></li>
<li class="chapter" data-level="4.3.2" data-path="collective-risk-model.html"><a href="collective-risk-model.html#compound-negative-binomial-distributions"><i class="fa fa-check"></i><b>4.3.2</b> Compound negative binomial distributions</a></li>
<li class="chapter" data-level="4.3.3" data-path="collective-risk-model.html"><a href="collective-risk-model.html#compound-binomial-distributions"><i class="fa fa-check"></i><b>4.3.3</b> Compound binomial distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="collective-risk-model.html"><a href="collective-risk-model.html#the-effect-of-reinsurance"><i class="fa fa-check"></i><b>4.4</b> The effect of reinsurance</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="collective-risk-model.html"><a href="collective-risk-model.html#proportional-reinsurance-1"><i class="fa fa-check"></i><b>4.4.1</b> Proportional reinsurance</a></li>
<li class="chapter" data-level="4.4.2" data-path="collective-risk-model.html"><a href="collective-risk-model.html#excess-of-loss-reinsurance-1"><i class="fa fa-check"></i><b>4.4.2</b> Excess of loss reinsurance</a></li>
<li class="chapter" data-level="4.4.3" data-path="collective-risk-model.html"><a href="collective-risk-model.html#compound-poisson-distributions-under-excess-of-loss-reinsurance"><i class="fa fa-check"></i><b>4.4.3</b> Compound Poisson distributions under excess of loss reinsurance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="tutorials.html"><a href="tutorials.html"><i class="fa fa-check"></i><b>5</b> Tutorials</a>
<ul>
<li class="chapter" data-level="5.1" data-path="tutorials.html"><a href="tutorials.html#tutorial-1"><i class="fa fa-check"></i><b>5.1</b> Tutorial 1</a></li>
<li class="chapter" data-level="5.2" data-path="tutorials.html"><a href="tutorials.html#tutorial-2"><i class="fa fa-check"></i><b>5.2</b> Tutorial 2</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interactive-lecture.html"><a href="interactive-lecture.html"><i class="fa fa-check"></i><b>6</b> Interactive Lecture</a>
<ul>
<li class="chapter" data-level="6.1" data-path="interactive-lecture.html"><a href="interactive-lecture.html#datacamp-light"><i class="fa fa-check"></i><b>6.1</b> DataCamp Light</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">SCMA470 Risk Analysis and Credibility</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">SCMA470 Risk Analysis and Credibility</h1>
<p class="author"><em>Pairote Satiracoo</em></p>
<p class="date" style="margin-top: 1.5em;"><em>2021-09-14</em></p>
</div>
<div id="basic-probability-concepts" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Basic Probability Concepts</h1>
<div id="random-variables" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Random Variables</h2>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 1.1  </strong></span><em>Let <span class="math inline">\(S\)</span> be the sample space of an experiment. A
real-valued function <span class="math inline">\(X : S \rightarrow \mathbb{R}\)</span> is called a <strong>random
variable</strong> of the experiment if, for each interval
<span class="math inline">\(I \subset \mathbb{R}, \, \{s : X(s) \in I \}\)</span> is an event. </em></p>
</div>
<p>Random variables are often used for the calculation of the probabilities
of events. The real-valued function <span class="math inline">\(P(X \le t)\)</span> characterizes <span class="math inline">\(X\)</span>, it
tells us almost everything about <span class="math inline">\(X\)</span>. This function is called the
<strong>cumulative distribution function</strong> of <span class="math inline">\(X\)</span>. The cumulative distribution
function describes how the probabilities accumulate.</p>
<div class="definition">
<p><span id="def:unlabeled-div-2" class="definition"><strong>Definition 1.2  </strong></span><em>If <span class="math inline">\(X\)</span> is a random variable, then the function <span class="math inline">\(F\)</span>
defined on <span class="math inline">\(\mathbb{R}\)</span> by <span class="math display">\[F(x) = P(X \le x)\]</span> is called the
<strong>cumulative distribution function</strong> or simply <strong>distribution function
(c.d.f)</strong> of <span class="math inline">\(X\)</span>.</em></p>
</div>
<p>Functions that define the probability measure for discrete and
continuous random variables are the probability mass function and the
probability density function.</p>
<div class="definition">
<p><span id="def:unlabeled-div-3" class="definition"><strong>Definition 1.3  </strong></span><em>Suppose <span class="math inline">\(X\)</span> is a discrete random variable. Then the
function <span class="math display">\[f(x) = P(X = x)\]</span> that is defined for each <span class="math inline">\(x\)</span> in the range
of <span class="math inline">\(X\)</span> is called the <strong>probability mass function</strong> (p.m.f) of a random
variable <span class="math inline">\(X\)</span>.</em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 1.4  </strong></span><em>Suppose <span class="math inline">\(X\)</span> is a continuous random variable with
c.d.f <span class="math inline">\(F\)</span> and there exists a nonnegative, integrable function <span class="math inline">\(f\)</span>,
<span class="math inline">\(f: \mathbb{R} \rightarrow [0, \infty)\)</span> such that
<span class="math display">\[F(x) = \int_{-\infty}^x f(y)\, dy\]</span> Then the function <span class="math inline">\(f\)</span> is called
the <strong>probability density function</strong> (p.d.f) of a random variable <span class="math inline">\(X\)</span>.</em></p>
</div>
<div id="r-functions-for-probability-distributions" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> R Functions for Probability Distributions</h3>
<p>In R, density, distribution function, for the Poisson distribution with parameter <span class="math inline">\(\lambda\)</span> is shown as follows:</p>
<table>
<colgroup>
<col width="12%" />
<col width="21%" />
<col width="22%" />
<col width="21%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th>Density function: <span class="math inline">\(P(X = x)\)</span></th>
<th>Distribution function: <span class="math inline">\(P(X ≤ x)\)</span></th>
<th>Quantile function (inverse c.d.f.)</th>
<th>random generation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Poisson</td>
<td><code>dpois(x, lambda, log = FALSE)</code></td>
<td><code>ppois(q, lambda, lower.tail = TRUE, log.p = FALSE)</code></td>
<td><code>qpois(p, lambda, lower.tail = TRUE, log.p = FALSE)</code></td>
<td><code>rpois(n, lambda)</code></td>
</tr>
</tbody>
</table>
<p>For the binomial distribution, these functions are pbinom, qbinom, dbinom, and rbinom. For the normal distribution, these functions are pnorm, qnorm, dnorm, and rnorm. And so forth.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="index.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-2"><a href="index.html#cb1-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span></span>
<span id="cb1-3"><a href="index.html#cb1-3" aria-hidden="true" tabindex="-1"></a>myData <span class="ot">&lt;-</span> <span class="fu">data.frame</span>( <span class="at">k =</span> <span class="fu">factor</span>(x), <span class="at">pK =</span> <span class="fu">dbinom</span>(x, <span class="dv">20</span>, .<span class="dv">5</span>))</span>
<span id="cb1-4"><a href="index.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(myData,<span class="fu">aes</span>(k,<span class="at">ymin=</span><span class="dv">0</span>,<span class="at">ymax=</span>pK)) <span class="sc">+</span> </span>
<span id="cb1-5"><a href="index.html#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;p(k)&quot;</span>) <span class="sc">+</span></span>
<span id="cb1-6"><a href="index.html#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_discrete</span>(<span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">20</span>,<span class="dv">5</span>)) <span class="sc">+</span>   </span>
<span id="cb1-7"><a href="index.html#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;p.m.f of binomial distribution&quot;</span>)</span></code></pre></div>
<p><img src="SCMA470Bookdownproj_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>To plot continuous probability distribution in R, we use stat_function to add the density function as its arguement. To specify a different mean or standard deviation, we use the <code>args</code> parameter to supply new values.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="index.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb2-2"><a href="index.html#cb2-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="at">by=</span><span class="fl">0.1</span>))</span>
<span id="cb2-3"><a href="index.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df) <span class="sc">+</span> </span>
<span id="cb2-4"><a href="index.html#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stat_function</span>(<span class="fu">aes</span>(x),<span class="at">fun=</span>dnorm, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>))  <span class="sc">+</span> </span>
<span id="cb2-5"><a href="index.html#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;x&quot;</span>, <span class="at">y =</span> <span class="st">&quot;f(x)&quot;</span>, </span>
<span id="cb2-6"><a href="index.html#cb2-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">&quot;Normal Distribution With Mean = 0 &amp; SD = 1&quot;</span>) </span></code></pre></div>
<p><img src="SCMA470Bookdownproj_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
</div>
<div id="expectation" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Expectation</h2>
<div class="definition">
<p><span id="def:unlabeled-div-5" class="definition"><strong>Definition 1.5  </strong></span><em>The <strong>expected value</strong> of a discrete random variable
<span class="math inline">\(X\)</span> with the set of possible values <span class="math inline">\(A\)</span> and probability mass function
<span class="math inline">\(f(x)\)</span> is defined by <span class="math display">\[\mathrm{E}(X) = \sum_{x \in A} x f(x)\]</span></em></p>
</div>
<p>The <strong>expected value</strong> of a random variable <span class="math inline">\(X\)</span> is also called the mean,
or the mathematical expectation, or simply the expectation of <span class="math inline">\(X\)</span>. It is
also occasionally denoted by <span class="math inline">\(\mathrm{E}[X]\)</span>, <span class="math inline">\(\mu_X\)</span>, or <span class="math inline">\(\mu\)</span>.</p>
<p>Note that if each value <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span> is weighted by <span class="math inline">\(f(x) = P(X = x)\)</span>,
then <span class="math inline">\(\displaystyle \sum_{x \in A} x f(x)\)</span> is nothing but the weighted
average of <span class="math inline">\(X\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-6" class="theorem"><strong>Theorem 1.1  </strong></span><em>Let <span class="math inline">\(X\)</span> be a discrete random variable with set of
possible values <span class="math inline">\(A\)</span> and probability mass function <span class="math inline">\(f(x)\)</span>, and let <span class="math inline">\(g\)</span> be
a real-valued function. Then <span class="math inline">\(g(X)\)</span> is a random variable with
<span class="math display">\[\mathrm{E}[g(X)] = \sum_{x \in A} g(x) f(x)\]</span> </em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-7" class="definition"><strong>Definition 1.6  </strong></span><em>If <span class="math inline">\(X\)</span> is a continuous random variable with
probability density function <span class="math inline">\(f\)</span> , the <strong>expected value</strong> of <span class="math inline">\(X\)</span> is
defined by <span class="math display">\[\mathrm{E}(X) = \int_{-\infty}^\infty x f(x)\, dx\]</span> </em></p>
</div>
<div class="theorem">
<ul>
<li></li>
<li>Let <span class="math inline">\(X\)</span> be a continuous random variable with
probability density function <span class="math inline">\(f (x)\)</span>; then for any function
<span class="math inline">\(h: \mathbb{R} \rightarrow \mathbb{R}\)</span>,
<span class="math display">\[\mathrm{E}[h(X)] = \int_{-\infty}^\infty h(x)\, f(x)\, dx\]</span> *</li>
</ul>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-9" class="theorem"><strong>Theorem 1.2  </strong></span><em>Let <span class="math inline">\(X\)</span> be a random variable. Let
<span class="math inline">\(h_1, h_2, . . . , h_n\)</span> be real-valued functions, and
<span class="math inline">\(a_1, a_2, \ldots, a_n\)</span> be real numbers. Then
<span class="math display">\[\mathrm{E}[a_1 h_1(X) + a_2 h_2(X) + \cdots + a_n h_n(X)] = a_1 \mathrm{E}[h_1(X)] + a_2 \mathrm{E}[h_2(X)] + \ldots +
    a_n \mathrm{E}[h_n(X)]\]</span></em></p>
</div>
<p>Moreover, if <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, then
<span class="math display">\[\mathrm{E}(aX +b) = a\mathrm{E}(x) + b\]</span></p>
</div>
<div id="variances-of-random-variables" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Variances of Random Variables</h2>
<div class="definition">
<p><span id="def:unlabeled-div-10" class="definition"><strong>Definition 1.7  </strong></span><em>Let <span class="math inline">\(X\)</span> be a discrete random variable with a set of
possible values <span class="math inline">\(A\)</span>, probability mass function <span class="math inline">\(f(x)\)</span>, and
<span class="math inline">\(\mathrm{E}(X) = \mu\)</span>. then <span class="math inline">\(\mathrm{Var}(X)\)</span> and <span class="math inline">\(\sigma_X\)</span>, called the
<strong>variance</strong> and <strong>standard deviation</strong> of <span class="math inline">\(X\)</span>, respectively, are
defined by
<span class="math display">\[\mathrm{Var}(X) = \mathrm{E}[(X- \mu)^2] = \sum_{x \in A} (x - \mu)^2 f(x),\]</span>
<span class="math display">\[\sigma_X = \sqrt{\mathrm{E}[(X- \mu)^2]}\]</span></em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-11" class="definition"><strong>Definition 1.8  </strong></span><em>If <span class="math inline">\(X\)</span> is a continuous random variable with
<span class="math inline">\(\mathrm{E}(X) = \mu\)</span>, then <span class="math inline">\(\mathrm{Var}(X)\)</span> and <span class="math inline">\(\sigma_X\)</span>, called the
<strong>variance</strong> and <strong>standard deviation</strong> of <span class="math inline">\(X\)</span>, respectively, are
defined by
<span class="math display">\[\mathrm{Var}(X) = \mathrm{E}[(X- \mu)^2] =  \int_{-\infty}^\infty (x - \mu)^2\, f(x)\, dx ,\]</span>
<span class="math display">\[\sigma_X = \sqrt{\mathrm{E}[(X- \mu)^2]}\]</span></em></p>
</div>
<p>We have the following important relations
<span class="math display">\[\mathrm{Var}(x) = \mathrm{E}(X^2) - (\mathrm{E}(x))^2 ,\]</span>
<span class="math display">\[\mathrm{Var}(aX + b) = a^2\ Var(X), \quad   \sigma_{aX + b}= |a|\sigma_X\]</span>
where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants.</p>
</div>
<div id="moments-and-moment-generating-function" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Moments and Moment Generating Function</h2>
<div class="definition">
<p><span id="def:unlabeled-div-12" class="definition"><strong>Definition 1.9  </strong></span><em>For <span class="math inline">\(r &gt; 0\)</span>, the <span class="math inline">\(r\)</span>th moment of <span class="math inline">\(X\)</span> (the <span class="math inline">\(r\)</span>th
moment about the origin) is <span class="math inline">\(\mathrm{E}[X^r]\)</span>, when it is defined. The
<span class="math inline">\(r\)</span>th central moment of a random variable <span class="math inline">\(X\)</span> (the <span class="math inline">\(r\)</span>th moment about
the mean) is <span class="math inline">\(\mathrm{E}[(X - \mathrm{E}[X])^r].\)</span> </em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-13" class="definition"><strong>Definition 1.10  </strong></span><em>The skewness of <span class="math inline">\(X\)</span> is defined to be the third
central moment, <span class="math display">\[\mathrm{E}[(X - \mathrm{E}[X])^3],\]</span> and the
coefficient of skewness to be given by
<span class="math display">\[\frac{\mathrm{E}[(X - \mathrm{E}[X])^3]}{(\mathrm{Var}[X])^{3/2}}.\]</span> </em></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-14" class="definition"><strong>Definition 1.11  </strong></span><em>The coefficient of kurtosis of <span class="math inline">\(X\)</span> is defined by
<span class="math display">\[\frac{\mathrm{E}[(X - \mathrm{E}[X])^4]}{(\mathrm{Var}[X])^{4/2}}.\]</span> </em></p>
</div>
<p><strong>Note</strong> In the formula, subtract from the mean and normalise or divide by the standard deviation center and scale to the standard values. Odd-order moments are increased if there is a long tail to the right and decreased if there is a long tail to the left, while even-order moments are increased if either tail is long. A negative value of the coefficient of skewness that the distribution is skewed to the left, or negatively skewed, meaning that the deviations above the mean tend to be smaller than the deviations below the mean, and vice versa. If the coefficent of skewness is close to zero, this could mean symmetry,</p>
<p><strong>Note</strong> The fourth moment measures the fatness in the tails, which is always positive. The kurtosis of the standard normal distribution is 3. Using the standard normal distribution as a benchmark, the excess kurtosis of a random variable is defined as the kurtosis minus 3. A higher kurtosis corresponds to a larger extremity of deviations (or outliers), which is called excess kurtosis.</p>
<p>The following diagram compares the shape between the normal distribution and Student’s t-distribution. Note that to use the legend with the <code>stat_function</code> in ggplot2, we use <code>scale_colour_manual</code> along with <code>colour =</code> inside the <code>aes()</code> as shown below and give names for specific density plots.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="index.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb3-2"><a href="index.html#cb3-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="at">by=</span><span class="fl">0.1</span>))</span>
<span id="cb3-3"><a href="index.html#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df) <span class="sc">+</span> </span>
<span id="cb3-4"><a href="index.html#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stat_function</span>(<span class="fu">aes</span>(x, <span class="at">colour =</span> <span class="st">&quot;dnorm&quot;</span>),<span class="at">fun =</span> dnorm, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>))  <span class="sc">+</span> </span>
<span id="cb3-5"><a href="index.html#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stat_function</span>(<span class="fu">aes</span>(x, <span class="at">colour =</span><span class="st">&quot;dt&quot;</span>),<span class="at">fun =</span> dt, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">df =</span> <span class="dv">4</span>)) <span class="sc">+</span></span>
<span id="cb3-6"><a href="index.html#cb3-6" aria-hidden="true" tabindex="-1"></a>     <span class="fu">scale_colour_manual</span>(<span class="st">&quot;Legend title&quot;</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>)) <span class="sc">+</span> </span>
<span id="cb3-7"><a href="index.html#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;x&quot;</span>, <span class="at">y =</span> <span class="st">&quot;f(x)&quot;</span>, </span>
<span id="cb3-8"><a href="index.html#cb3-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">&quot;Normal Distribution With Mean = 0 &amp; SD = 1&quot;</span>) <span class="sc">+</span> </span>
<span id="cb3-9"><a href="index.html#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p><img src="SCMA470Bookdownproj_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Next we will simulate 10000 samples from a normal distribution with mean 0, and standard deviation 1, then compute and interpret for the skewness and kurtosis, and plot the histogram. Here we also use
the function <code>set.seed()</code> to set the seed of R’s random number generator, this is useful for creating simulations or random objects that can be reproduced.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="index.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">15</span>) <span class="co"># Set the seed of R&#39;s random number generator</span></span>
<span id="cb4-2"><a href="index.html#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="index.html#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Simulation</span></span>
<span id="cb4-4"><a href="index.html#cb4-4" aria-hidden="true" tabindex="-1"></a>n.sample <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">10000</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb4-5"><a href="index.html#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="index.html#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Skewness and Kurtosis</span></span>
<span id="cb4-7"><a href="index.html#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(moments)</span>
<span id="cb4-8"><a href="index.html#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">skewness</span>(n.sample)</span></code></pre></div>
<pre><code>## [1] -0.03585812</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="index.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kurtosis</span>(n.sample)</span></code></pre></div>
<pre><code>## [1] 2.963189</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="index.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> n.sample),<span class="fu">aes</span>(x)) <span class="sc">+</span> </span>
<span id="cb8-2"><a href="index.html#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="SCMA470Bookdownproj_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="index.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">15</span>)</span>
<span id="cb9-2"><a href="index.html#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="index.html#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Simulation</span></span>
<span id="cb9-4"><a href="index.html#cb9-4" aria-hidden="true" tabindex="-1"></a>t.sample <span class="ot">&lt;-</span> <span class="fu">rt</span>(<span class="at">n =</span> <span class="dv">10000</span>, <span class="at">df =</span> <span class="dv">5</span>)</span>
<span id="cb9-5"><a href="index.html#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="index.html#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Skewness and Kurtosis</span></span>
<span id="cb9-7"><a href="index.html#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(moments)</span>
<span id="cb9-8"><a href="index.html#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="fu">skewness</span>(t.sample)</span></code></pre></div>
<pre><code>## [1] 0.06196269</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="index.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kurtosis</span>(t.sample)</span></code></pre></div>
<pre><code>## [1] 7.646659</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="index.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> t.sample),<span class="fu">aes</span>(x)) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="SCMA470Bookdownproj_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><strong>Example</strong> Let us count the number of samples greater than 5 from the samples of the normal and Student’s t distributions. Comment on your results</p>
<script src=https://cdn.datacamp.com/datacamp-light-latest.min.js></script>
<div data-datacamp-exercise="" data-height="300" data-encoded="true">
eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFdyaXRlIHlvdXIgY29kZSBoZXJlXG5zZXQuc2VlZCgxNSlcbm4uc2FtcGxlIDwtIHJub3JtKG4gPSAxMDAwMCwgbWVhbiA9IDAsIHNkID0gMSlcbnQuc2FtcGxlIDwtIHJ0KG4gPSAxMDAwMCwgZGYgPSA1KSJ9
</div>
<div class="definition">
<p><span id="def:unlabeled-div-15" class="definition"><strong>Definition 1.12  </strong></span><em>The moment generating function (mgf) of a random
variable <span class="math inline">\(X\)</span> is defined to be <span class="math display">\[M_X(t) = E[e^{tX}],\]</span> if the expectation
exists.</em></p>
</div>
<p><strong>Note</strong> The moment generating function of <span class="math inline">\(X\)</span> may not defined (may not
be finite) for all <span class="math inline">\(t\)</span> in <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>If <span class="math inline">\(M_X(t)\)</span> is finite for <span class="math inline">\(|t| &lt; h\)</span> for some <span class="math inline">\(h &gt; 0\)</span>, then, for any
<span class="math inline">\(k = 1, 2, \ldots,\)</span> the function <span class="math inline">\(M_X(t)\)</span> is k-times differentiable at
<span class="math inline">\(t = 0\)</span>, with <span class="math display">\[M^{(k)}_X (0) = \mathrm{E}[X^k],\]</span> with
<span class="math inline">\(\mathrm{E}[|X|^k]\)</span> finite. We can obtain the moments by succesive
differentiation of <span class="math inline">\(M_X(t)\)</span> and letting <span class="math inline">\(t = 0\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-16" class="example"><strong>Example 1.1  </strong></span>Derive the formula for the mgf of the standard normal distribution.
Hint: its mgf is <span class="math inline">\(e^{\frac{1}{2} t^2}\)</span>.</p>
</div>
</div>
<div id="probability-generating-function" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Probability generating function</h2>
<div class="definition">
<p><span id="def:unlabeled-div-17" class="definition"><strong>Definition 1.13  </strong></span><em>For a counting variable <span class="math inline">\(N\)</span> (a variable which
assumes some or all of the values <span class="math inline">\(0, 1, 2, \ldots,\)</span> but no others), The
probability generating function of <span class="math inline">\(N\)</span> is <span class="math display">\[G_N(t) = E[t^N],\]</span> for those
<span class="math inline">\(t\)</span> in <span class="math inline">\(\mathbb{R}\)</span> for which the series converges absolutely. </em></p>
</div>
<p>Let <span class="math inline">\(p_k = P(N = k)\)</span>. Then
<span class="math display">\[G_N(t) = E[t^N] = \sum_{k=0}^\infty t^k p_k.\]</span> It can be shown that if
<span class="math inline">\(E[N] &lt; \infty\)</span> then <span class="math display">\[\mathrm{E}[N] = G&#39;_N(1),\]</span> and if
<span class="math inline">\(E[N^2] &lt; \infty\)</span> then
<span class="math display">\[\mathrm{Var}[N] = G&#39;&#39;_N(1) + G&#39;_N(1) - (G&#39;_N(1))^2.\]</span> Moreover, when
both pgf and mgf of <span class="math inline">\(N\)</span> are defined, we have
<span class="math display">\[G_N(t) = M_N(\log(t)) \quad \text{ and } M_N(t) = G_N(e^t).\]</span></p>
</div>
<div id="multivariate-distributions" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Multivariate Distributions</h2>
<p>When <span class="math inline">\(X_1,X_2,\ldots ,X_n\)</span> be random variables defined on the same
sample space, a multivariate probability density function or probability
mass function<br />
<span class="math inline">\(f(x_1, x_2, \ldots x_n)\)</span> can be defined. The following definitions can
be extended to more than two random variables and the case of discrete
random variables.</p>
<div class="definition">
<p><span id="def:unlabeled-div-18" class="definition"><strong>Definition 1.14  </strong></span><em>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, defined on the
same sample space, have a continuous joint distribution if there exists
a nonnegative function of two variables, <span class="math inline">\(f(x, y)\)</span> on
<span class="math inline">\(\mathbb{R} \times \mathbb{R}\)</span> , such that for any region <span class="math inline">\(R\)</span> in the
<span class="math inline">\(xy\)</span>-plane that can be formed from rectangles by a countable number of
set operations, <span class="math display">\[P((X, Y)  \in R) = \iint_R f(x,y) \, dx\, dy\]</span> </em></p>
</div>
<p>The function <span class="math inline">\(f (x, y)\)</span> is called the <strong>joint probability density
function</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have joint probability density function <span class="math inline">\(f (x, y)\)</span>. Let
<span class="math inline">\(f_Y\)</span> be the probability density function of <span class="math inline">\(Y\)</span> . To find <span class="math inline">\(f_Y\)</span> in
terms of <span class="math inline">\(f\)</span> , note that, on the one hand, for any subset <span class="math inline">\(B\)</span> of <span class="math inline">\(R\)</span>,
<span class="math display">\[P(Y \in B) = \int_B f_Y(y) \, dy,\]</span> and on the other hand, we also
have
<span class="math display">\[P(Y \in B) = P(X \in (-\infty, \infty), Y \in B) = \int_B \left(    \int_{-\infty}^\infty f(x,y)\, dx  \right) \, dy.\]</span></p>
<p>We have
<span class="math display" id="eq:label">\[\begin{equation} 
\tag{1.1} f_Y(y) =   \int_{-\infty}^\infty f(x,y)\, dx
\end{equation}\]</span>
and
<span class="math display" id="eq:label2">\[\begin{equation} 
\tag{1.2}  f_X(x) =   \int_{-\infty}^\infty f(x,y)\, dy
\end{equation}\]</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-19" class="definition"><strong>Definition 1.15  </strong></span><em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have joint probability density
function <span class="math inline">\(f (x, y)\)</span>; then the functions <span class="math inline">\(f_X\)</span> and <span class="math inline">\(f_Y\)</span> in</em> <a href="index.html#eq:label">(1.1)</a> and <a href="index.html#eq:label2">(1.2)</a>
<em>are called,
respectively, the <strong>marginal probability density functions</strong> of <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span> .</em></p>
</div>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables (discrete, continuous, or
mixed). The <strong>joint probability distribution function</strong>, or <strong>joint
cumulative probability distribution function</strong>, or simply the joint
distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, is defined by
<span class="math display">\[F(t, u) = P(X \le t, Y \le u)\]</span> for all <span class="math inline">\(t, u \in (-\infty, \infty)\)</span>.</p>
<p>The marginal probability distribution function of <span class="math inline">\(X\)</span>, <span class="math inline">\(F_X\)</span>, can be
found from <span class="math inline">\(F\)</span> as follows:
<span class="math display">\[F_X(t) = \lim_{n \rightarrow \infty} F(t,u)  = F(t, \infty)\]</span> and
<span class="math display">\[F_Y(u) = \lim_{n \rightarrow \infty}F(t,u)  = F( \infty, u)\]</span></p>
<p>The following relationship between <span class="math inline">\(f(x,y)\)</span> and <span class="math inline">\(F(t,u)\)</span> is as follows:
<span class="math display">\[F(t,u) = \int_{-\infty}^{u}\int_{-\infty}^{t} f(x,y)\, dx\, dy.\]</span></p>
<p>We also have
<span class="math display">\[\mathrm{E}(X) =   \int_{-\infty}^\infty x f_X(x)\, dx , \quad \mathrm{E}(Y) =   \int_{-\infty}^\infty y f_Y(y)\, dy\]</span></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-20" class="theorem"><strong>Theorem 1.3  </strong></span><em>Let <span class="math inline">\(f (x, y)\)</span> be the joint probability density function
of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. If <span class="math inline">\(h\)</span> is a function of two variables
from <span class="math inline">\(\mathbb{R}^2\)</span> to <span class="math inline">\(\mathbb{R}\)</span>, then <span class="math inline">\(h(X, Y )\)</span> is a random
variable with the expected value given by
<span class="math display">\[\mathrm{E}[h(X,Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} h(x,y) \, f(x,y)\, dx\, dy\]</span>
provided that the integral is absolutely convergent.</em></p>
</div>
<p>As a consequence of the above theorem, for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,
<span class="math display">\[\mathrm{E}(X + Y) = \mathrm{E}(X) + \mathrm{E}(Y)\]</span></p>
</div>
<div id="independent-random-variables" class="section level2" number="1.7">
<h2><span class="header-section-number">1.7</span> Independent random variables</h2>
<div class="definition">
<p><span id="def:unlabeled-div-21" class="definition"><strong>Definition 1.16  </strong></span><em>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are called
independent if, for arbitrary subsets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> of real numbers, the
events <span class="math inline">\(\{X \in A\}\)</span> and <span class="math inline">\(\{Y \in B\}\)</span> are <strong>independent</strong>, that is, if
<span class="math display">\[P(X \in A, Y \in B) = P(X \in A) P(Y \in B).\]</span></em></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-22" class="theorem"><strong>Theorem 1.4  </strong></span><em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables defined on the
same sample space. If <span class="math inline">\(F\)</span> is the joint probability distribution function
of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if and only if for all
real numbers <span class="math inline">\(t\)</span> and <span class="math inline">\(u\)</span>, <span class="math display">\[F(t,u)  = F_X(t) F_Y(u).\]</span></em></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-23" class="theorem"><strong>Theorem 1.5  </strong></span><em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be jointly continuous random variables
with joint probability density function <span class="math inline">\(f (x, y)\)</span>. Then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are
independent if and only if <span class="math display">\[f (x, y) = f_X(x) f_Y (y).\]</span></em></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-24" class="theorem"><strong>Theorem 1.6  </strong></span><em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables and
<span class="math inline">\(g : \mathbb{R} \rightarrow\mathbb{R}\)</span> and
<span class="math inline">\(h : \mathbb{R} \rightarrow\mathbb{R}\)</span> be real-valued functions; then
<span class="math inline">\(g(X)\)</span> and <span class="math inline">\(h(Y )\)</span> are also independent random variables.</em></p>
</div>
<p>As a consequence of the above theorem, we obtain</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-25" class="theorem"><strong>Theorem 1.7  </strong></span><em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables. Then
for all real-value functions <span class="math inline">\(g : \mathbb{R} \rightarrow\mathbb{R}\)</span> and
<span class="math inline">\(h : \mathbb{R} \rightarrow\mathbb{R}\)</span>,
<span class="math display">\[\mathrm{E}[g(X)h(Y)] = \mathrm{E}[g(X)]\mathrm{E}[h(Y)]\]</span> </em></p>
</div>
</div>
<div id="conditional-distributions" class="section level2" number="1.8">
<h2><span class="header-section-number">1.8</span> Conditional Distributions</h2>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two continuous random variables with the joint
probability density function <span class="math inline">\(f (x, y)\)</span>. Note that the case of discrete
random variables can be considered in the same way. When no information
is given about the value of <span class="math inline">\(Y\)</span>, the marginal probability density
function of <span class="math inline">\(X\)</span>, <span class="math inline">\(f_X(x)\)</span> is used to calculate the probabilities of
events concerning <span class="math inline">\(X\)</span>. However, when the value of <span class="math inline">\(Y\)</span> is known, to find
such probabilities, <span class="math inline">\(f_{X|Y} (x|y)\)</span>, the conditional probability density
function of <span class="math inline">\(X\)</span> given that <span class="math inline">\(Y = y\)</span> is used and is defined as follows:
<span class="math display">\[f_{X|Y} (x|y)  = \frac{f(x,y)}{f_Y(y)}\]</span> provided that <span class="math inline">\(f_Y (y) &gt; 0\)</span>.
Note also that the conditional probability density function of <span class="math inline">\(X\)</span> given
that <span class="math inline">\(Y = y\)</span> is itseof a probability density function, i.e.
<span class="math display">\[\int_{-\infty}^\infty f_{X|Y}(x|y)\, dx  =  1.\]</span></p>
<p>Note that the conditional probability distribution function of <span class="math inline">\(X\)</span> given
that <span class="math inline">\(Y = y\)</span>, the conditional expectation of <span class="math inline">\(X\)</span> given that <span class="math inline">\(Y = y\)</span> can
be as follows:
<span class="math display">\[F_{Y|X}(x|y) = P(X \le x | Y = y) = \int_ {-\infty}^x f_{X|Y}(t|y) \, dt\]</span>
and
<span class="math display">\[\mathrm{E}(X|Y = y) =  \int_{-\infty}^{\infty} x  f_{X|Y}(x|y) \, dx,\]</span>
where <span class="math inline">\(f_Y(y) &gt; 0\)</span>.</p>
<p>Note that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(f_{X|Y}\)</span> coincides with
<span class="math inline">\(f_X\)</span> because
<span class="math display">\[f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)} =\frac{f_X(x)f_Y(y)}{f_Y(y)} = f_X(x).\]</span></p>
</div>
<div id="covariance" class="section level2" number="1.9">
<h2><span class="header-section-number">1.9</span> Covariance</h2>
<p>The notion of the variance of a random variable <span class="math inline">\(X\)</span>,
<span class="math inline">\(\mathrm{Var}(X) = \mathrm{E}[ ( X - \mathrm{E}(X))^2]\)</span> measures the
average magnitude of the fluctuations of the random variable <span class="math inline">\(X\)</span> from
its expectation, <span class="math inline">\(\mathrm{E}(X)\)</span>. This quantity measures the dispersion,
or spread, of the distribution of <span class="math inline">\(X\)</span> about its expectation. Now suppose
that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two jointly distributed random variables.
Covariance is a measure of how much two random variables vary together.</p>
<p>Let us calculuate <span class="math inline">\(\mathrm{Var}(aX + bY)\)</span> the joint spread, or
dispersion, of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> along the <span class="math inline">\((ax + by)\)</span>-direction for arbitrary
real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:
<span class="math display">\[\mathrm{Var}(aX + bY) = a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) + 2 a b \mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))].\]</span>
However, <span class="math inline">\(\mathrm{Var}(X)\)</span> and <span class="math inline">\(\mathrm{Var}(Y )\)</span> determine the
dispersions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> independently; therefore,
<span class="math inline">\(\mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))]\)</span> is the quantity
that gives information about the joint spread, or dispersion, <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span> .</p>
<div class="definition">
<p><span id="def:unlabeled-div-26" class="definition"><strong>Definition 1.17  </strong></span><em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be jointly distributed random
variables; then the <strong>covariance</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined by
<span class="math display">\[\mathrm{Cov}(X,Y) =  \mathrm{E}[(X - \mathrm{E}(X))(Y - \mathrm{E}(Y))].\]</span></em></p>
</div>
<p>Note that for random variables <span class="math inline">\(X, Y\)</span> and <span class="math inline">\(Z\)</span>, and <span class="math inline">\(ab &gt; 0\)</span>, then the
joint dispersion of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> along the <span class="math inline">\((ax + by)\)</span>-direction is
greater than the joint dispersion of <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> along the
<span class="math inline">\((ax + bz)\)</span>-direction if and only if
<span class="math inline">\(\mathrm{Cov}(X, Y) &gt; \mathrm{Cov}(X,Z).\)</span></p>
<p>Note that <span class="math display">\[\mathrm{Cov}(X, X) = \mathrm{Var}(X).\]</span> Moreover,
<span class="math display">\[\mathrm{Cov}(X,Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y).\]</span></p>
<p>Properties of covariance are as follows: for arbitrary real numbers
<span class="math inline">\(a, b, c, d\)</span> and random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,
<span class="math display">\[\mathrm{Var}(aX + bY) = a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) + 2 a b \mathrm{Cov}(X,Y).\]</span>
<span class="math display">\[\mathrm{Cov}(aX + b, cY + d) = acCov(X, Y)\]</span> For random variables
<span class="math inline">\(X_1, X_2, . . . , X_n\)</span> and <span class="math inline">\(Y_1, Y_2, . . . , Y_m\)</span>,
<span class="math display">\[\mathrm{Cov}(\sum_{i=1}^n a_i X_i, \sum_{j=1}^m b_j Y_j) = \sum_{i=1}^n\sum_{j=1}^m a_i\,b_j\, \mathrm{Cov}(X_i,Y_j).\]</span></p>
<p>If <span class="math inline">\(\mathrm{Cov}(X, Y) &gt; 0\)</span>, we say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are positively
correlated. If <span class="math inline">\(\mathrm{Cov}(X, Y) &lt; 0\)</span>, we say that they are negatively
correlated. If <span class="math inline">\(\mathrm{Cov}(X, Y) = 0\)</span>, we say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are
uncorrelated.</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math display">\[\mathrm{Cov}(X,Y) = 0.\]</span> However,
the converse of this is not true; that is, two dependent random
variables might be uncorrelated.</p>
</div>
<div id="correlation" class="section level2" number="1.10">
<h2><span class="header-section-number">1.10</span> Correlation</h2>
<p>A large covariance can mean a strong relationship between variables.
However, we cannot compare variances over data sets with different
scales. A weak covariance in one data set may be a strong one in a
different data set with different scales. The problem can be fixed by
dividing the covariance by the standard deviation to get the correlation
coefficient.</p>
<div class="definition">
<p><span id="def:unlabeled-div-27" class="definition"><strong>Definition 1.18  </strong></span><em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables with
<span class="math inline">\(0&lt; \sigma^2_X, \sigma^2_Y &lt; \infty\)</span>. The covariance between the
standardized <span class="math inline">\(X\)</span> and the standardized <span class="math inline">\(Y\)</span> is called the correlation
coefficient between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and is denoted <span class="math inline">\(\rho = \rho(X,Y)\)</span>,
<span class="math display">\[\rho(X,Y) = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}.\]</span> </em></p>
</div>
<p>Note that</p>
<ul>
<li><p><span class="math inline">\(\rho(X, Y ) &gt; 0\)</span> if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are positively
correlated;</p></li>
<li><p><span class="math inline">\(\rho(X, Y ) &lt; 0\)</span> if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are negatively
correlated; and</p></li>
<li><p><span class="math inline">\(\rho(X, Y ) = 0\)</span> if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated.</p></li>
<li><p><span class="math inline">\(\rho(X, Y )\)</span> roughly measures the amount and the sign of linear
relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<p>In the case of perfect linear relationship, we have
<span class="math inline">\(\rho(X, Y ) = \pm1\)</span>. A correlation of 0, i.e. <span class="math inline">\(\rho(X, Y ) = 0\)</span> does
not mean zero relationship between two variables; rather, it means zero
linear relationship.</p>
<p>Some importants properties of correlation are
<span class="math display">\[-1 \le \rho(X, Y ) \le 1\]</span>
<span class="math display">\[\rho(a X + b, cY +d) = \text{sign}(ac) \rho(X, Y )\]</span></p>
</div>
<div id="model-fitting" class="section level2" number="1.11">
<h2><span class="header-section-number">1.11</span> Model Fitting</h2>
<p>The contents in this section are taken from Gray and Pitts.</p>
<p>To fit a parametric model, we have to calculate estimates of the unknown
parameters of the probability distribution. Various criteria are
available, including the method of moments, the method of maximum
likelihood, etc.</p>
</div>
<div id="the-method-of-moments" class="section level2" number="1.12">
<h2><span class="header-section-number">1.12</span> The method of moments</h2>
<p>The method of moments leads to parameter estimates by simply matching
the moments of the model,
<span class="math inline">\(\mathrm{E}[X], \mathrm{E}[X^2], \mathrm{E}[X^3], \ldots ,\)</span> in turn to
the required number of corresponding sample moments calculated from the
data <span class="math inline">\(x_1, x_2, \ldots , x_n\)</span>, where <span class="math inline">\(n\)</span> is the number of observations
available. The sample moments are simply
<span class="math display">\[\frac{1}{n}\sum_{i=1}^n x_i, \quad  \frac{1}{n}\sum_{i=1}^n  x^2_i, \quad \frac{1}{n}\sum_{i=1}^n x^3_i, \ldots.\]</span>
It is often more convenient to match the mean and central moments, in
particular matching <span class="math inline">\(\mathrm{E}[X]\)</span> to the sample mean <span class="math inline">\(\bar{x}\)</span> and
<span class="math inline">\(\mathrm{Var}[X]\)</span> to the sample variance
<span class="math display">\[s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2.\]</span></p>
<p>An estimate produced using the method of moments is called an MME, and
the MME of a parameter <span class="math inline">\(\theta\)</span>, say, is usually denoted
<span class="math inline">\(\tilde{\theta}\)</span>.</p>
</div>
<div id="the-method-of-maximum-likelihood" class="section level2" number="1.13">
<h2><span class="header-section-number">1.13</span> The method of maximum likelihood</h2>
<p>The method of maximum likelihood is the most widely used method for
parameter estimation. The estimates it produces are those values of the
parameters which give the maximum value attainable by the likelihood
function, denoted <span class="math inline">\(L\)</span>, which is the joint probability mass or density
function for the data we have (under the chosen parametric
distribution), regarded as a function of the unknown parameters.</p>
<p>In practice, it is often easier to maximise the loglikelihood function,
which is the logarithm of the likelihood function, rather than the
likelihood itself. An estimate produced using the method of maximum
likelihood is called an MLE, and the MLE of a parameter <span class="math inline">\(\theta\)</span>, say,
is denoted <span class="math inline">\(\hat{\theta}\)</span>. MLEs have many desirable theoretical
properties, especially in the case of large samples.</p>
<p>In some simple cases we can derive MLE(s) analytically as explicit
functions of summaries of the data. Thus, suppose our data consist of a
random sample <span class="math inline">\(x_1, x_2, \ldots , x_n\)</span>, from a parametric distribution
whose parameter(s) we want to estimate. Some straightforward cases
include the following:</p>
<ul>
<li><p>the MLE of <span class="math inline">\(\lambda\)</span> for a <span class="math inline">\(Poi(\lambda)\)</span> distribution is the sample
mean, that is <span class="math inline">\(\hat{\lambda} = \bar{x}\)</span></p></li>
<li><p>the MLE of <span class="math inline">\(\lambda\)</span> for an <span class="math inline">\(Exp(\lambda)\)</span> distribution is the
reciprocal of the sample mean, that is <span class="math inline">\(\hat{\lambda} = 1/\bar{x}\)</span></p></li>
</ul>
</div>
<div id="goodness-of-fit-tests" class="section level2" number="1.14">
<h2><span class="header-section-number">1.14</span> Goodness of fit tests</h2>
<p>We can assess how well the fitted distributions reflect the distribution
of the data in various ways. We should, of course, examine and compare
the tables of frequencies and, if appropriate, plot and compare
empirical distribution functions. More formally, we can perform certain
statistical tests. Here we will use the Pearson chi-square
goodness-of-fit criterion.</p>
</div>
<div id="the-pearson-chi-square-goodness-of-fit-criterion" class="section level2" number="1.15">
<h2><span class="header-section-number">1.15</span> the Pearson chi-square goodness-of-fit criterion</h2>
<p>We construct the test statistic <span class="math display">\[\chi^2 = \frac{\sum(O - E)^2}{E},\]</span>
where <span class="math inline">\(O\)</span> is the observed frequency in a cell in the frequency table and
<span class="math inline">\(E\)</span> is the fitted or expected frequency (the frequency expected in that
cell under the fitted model), and where we sum over all usable cells.</p>
<p><strong>The null hypothesis</strong> is that the sample comes from a specified
distribution.</p>
<p>The value of the test statistic is then evaluated in one of two ways.</p>
<ol style="list-style-type: decimal">
<li><p>We convert it to a <span class="math inline">\(P\)</span>-value, which is a measure of the strength of
the evidence against the hypothesis that the data do follow the
fitted distribution. <strong>If the <span class="math inline">\(P\)</span>-value is small enough, we conclude
that the data do not follow the fitted distribution – we say “the
fitted distribution does not provide a good fit to the data” (and
quote the <span class="math inline">\(P\)</span>-value in support of this conclusion)</strong>.</p></li>
<li><p>We compare it with values in published tables of the distribution
function of the appropriate <span class="math inline">\(\chi^2\)</span> distribution, and if the value
of the statistic is high enough to be in a tail of specified size of
this reference distribution, we conclude that the fitted
distribution does not provide a good fit to the data.</p></li>
</ol>
</div>
<div id="kolmogorov-smirnov-k-s-test." class="section level2" number="1.16">
<h2><span class="header-section-number">1.16</span> Kolmogorov-Smirnov (K-S) test.</h2>
<p>The K-S test statistic is the maximum difference between the values of
the ecdf of the sample and the cdf of the fully specified fitted
distribution.</p>
<p>The course does not emphasis on the Goodness of Fit Test. Please refer
to the reference text for more details.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="loss-distributions.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pairote-sat/SCMA470/edit/master/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/pairote-sat/SCMA470/blob/master/index.Rmd",
"text": null
},
"download": ["SCMA470Bookdownproj.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
